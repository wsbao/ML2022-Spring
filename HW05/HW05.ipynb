{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFEKWoh3p1Mv"
      },
      "source": [
        "# Homework Description\n",
        "- English to Chinese (Traditional) Translation\n",
        "  - Input: an English sentence         (e.g.\t\ttom is a student .)\n",
        "  - Output: the Chinese translation  (e.g. \t\t湯姆 是 個 學生 。)\n",
        "\n",
        "- TODO\n",
        "    - Train a simple RNN seq2seq to acheive translation\n",
        "    - Switch to transformer model to boost performance\n",
        "    - Apply Back-translation to furthur boost performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3Vf1Q79XPQ3D"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Apr 15 01:19:50 2023       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 531.41                 Driver Version: 531.41       CUDA Version: 12.1     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA GeForce RTX 3080 L...  WDDM | 00000000:01:00.0  On |                  N/A |\n",
            "| N/A   45C    P8               19W /  N/A|   1184MiB / 16384MiB |     10%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|    0   N/A  N/A     10744    C+G   ...am Files\\Microsoft VS Code\\Code.exe    N/A      |\n",
            "|    0   N/A  N/A     13336    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
            "|    0   N/A  N/A     19272    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
            "|    0   N/A  N/A     30040    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
            "|    0   N/A  N/A     32564    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
            "|    0   N/A  N/A     35384    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
            "|    0   N/A  N/A     41440    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
            "|    0   N/A  N/A     41696    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
            "|    0   N/A  N/A     43876    C+G   ...__8wekyb3d8bbwe\\WindowsTerminal.exe    N/A      |\n",
            "|    0   N/A  N/A     45600    C+G   ...1.0_x64__8wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
            "|    0   N/A  N/A     49376    C+G   ...m Files\\Mozilla Firefox\\firefox.exe    N/A      |\n",
            "|    0   N/A  N/A     50136    C+G   ...m Files\\Mozilla Firefox\\firefox.exe    N/A      |\n",
            "|    0   N/A  N/A     51848    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
            "|    0   N/A  N/A     52248    C+G   ...7.0_x64__w2gh52qy24etm\\Nahimic3.exe    N/A      |\n",
            "|    0   N/A  N/A     52668    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59neB_Sxp5Ub"
      },
      "source": [
        "# Download and import required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rRlFbfFRpZYT"
      },
      "outputs": [],
      "source": [
        "# !pip install 'torch>=1.6.0' editdistance matplotlib sacrebleu sacremoses sentencepiece tqdm wandb\n",
        "# !pip install --upgrade jupyter ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fSksMTdmp-Wt"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/pytorch/fairseq.git\n",
        "# !cd fairseq && git checkout 9a1c497\n",
        "# !pip install --upgrade ./fairseq/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uRLTiuIuqGNc"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import pdb\n",
        "import pprint\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "import numpy as np\n",
        "import tqdm.auto as tqdm\n",
        "from pathlib import Path\n",
        "from argparse import Namespace\n",
        "from fairseq import utils\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n07Za1XqJzA"
      },
      "source": [
        "# Fix random seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xllxxyWxqI7s"
      },
      "outputs": [],
      "source": [
        "seed = 73\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  \n",
        "np.random.seed(seed)  \n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5ORDJ-2qdYw"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "## En-Zh Bilingual Parallel Corpus\n",
        "* [TED2020](#reimers-2020-multilingual-sentence-bert)\n",
        "    - Raw: 398,066 (sentences)   \n",
        "    - Processed: 393,980 (sentences)\n",
        "    \n",
        "\n",
        "## Testdata\n",
        "- Size: 4,000 (sentences)\n",
        "- **Chinese translation is undisclosed. The provided (.zh) file is psuedo translation, each line is a '。'**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQw2mY4Dqkzd"
      },
      "source": [
        "## Dataset Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SXT42xQtqijD"
      },
      "outputs": [],
      "source": [
        "data_dir = './DATA/rawdata'\n",
        "dataset_name = 'ted2020'\n",
        "# urls = (\n",
        "#     \"https://github.com/yuhsinchan/ML2022-HW5Dataset/releases/download/v1.0.2/ted2020.tgz\",\n",
        "#     \"https://github.com/yuhsinchan/ML2022-HW5Dataset/releases/download/v1.0.2/test.tgz\",\n",
        "# )\n",
        "# file_names = (\n",
        "#     'ted2020.tgz', # train & dev\n",
        "#     'test.tgz', # test\n",
        "# )\n",
        "prefix = Path(data_dir).absolute() / dataset_name\n",
        "\n",
        "# prefix.mkdir(parents=True, exist_ok=True)\n",
        "# for u, f in zip(urls, file_names):\n",
        "#     path = prefix/f\n",
        "#     if not path.exists():\n",
        "#         !wget {u} -O {path}\n",
        "#     if path.suffix == \".tgz\":\n",
        "#         !tar -xvf {path} -C {prefix}\n",
        "#     elif path.suffix == \".zip\":\n",
        "#         !unzip -o {path} -d {prefix}\n",
        "# !mv {prefix/'raw.en'} {prefix/'train_dev.raw.en'}\n",
        "# !mv {prefix/'raw.zh'} {prefix/'train_dev.raw.zh'}\n",
        "# !mv {prefix/'test/test.en'} {prefix/'test.raw.en'}\n",
        "# !mv {prefix/'test/test.zh'} {prefix/'test.raw.zh'}\n",
        "# !rm -rf {prefix/'test'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLkJwNiFrIwZ"
      },
      "source": [
        "## Language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_uJYkCncrKJb"
      },
      "outputs": [],
      "source": [
        "src_lang = 'zh' # 'en'\n",
        "tgt_lang = 'en' # 'zh'\n",
        "\n",
        "data_prefix = os.path.join(prefix, \"train_dev.raw\") # f'{prefix}/train_dev.raw'\n",
        "test_prefix = os.path.join(prefix, \"test.raw\") # f'{prefix}/test.raw'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0t2CPt1brOT3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "非常謝謝你，克里斯。能有這個機會第二度踏上這個演講台\n",
            "真是一大榮幸。我非常感激。\n",
            "這個研討會給我留下了極為深刻的印象，我想感謝大家 對我之前演講的好評。\n",
            "我是由衷的想這麼說，有部份原因是因為 —— 我真的有需要!\n",
            "請你們設身處地為我想一想！\n",
            "Thank you so much, Chris.\n",
            "And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\n",
            "I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.\n",
            "And I say that sincerely, partly because  I need that.\n",
            "Put yourselves in my position.\n"
          ]
        }
      ],
      "source": [
        "!head \"{data_prefix+'.'+src_lang}\" -n 5\n",
        "!head \"{data_prefix+'.'+tgt_lang}\" -n 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRoE9UK7r1gY"
      },
      "source": [
        "## Preprocess files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3tzFwtnFrle3"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def strQ2B(ustring):\n",
        "    \"\"\"Full width -> half width\"\"\"\n",
        "    # reference:https://ithelp.ithome.com.tw/articles/10233122\n",
        "    ss = []\n",
        "    for s in ustring:\n",
        "        rstring = \"\"\n",
        "        for uchar in s:\n",
        "            inside_code = ord(uchar)\n",
        "            if inside_code == 12288:  # Full width space: direct conversion\n",
        "                inside_code = 32\n",
        "            elif (inside_code >= 65281 and inside_code <= 65374):  # Full width chars (except space) conversion\n",
        "                inside_code -= 65248\n",
        "            rstring += chr(inside_code)\n",
        "        ss.append(rstring)\n",
        "    return ''.join(ss)\n",
        "                \n",
        "def clean_s(s, lang):\n",
        "    if lang == 'en':\n",
        "        s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n",
        "        s = s.replace('-', '') # remove '-'\n",
        "        s = re.sub('([.,;!?()\\\"])', r' \\1 ', s) # keep punctuation\n",
        "    elif lang == 'zh':\n",
        "        s = strQ2B(s) # Q2B\n",
        "        s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n",
        "        s = s.replace(' ', '')\n",
        "        s = s.replace('—', '')\n",
        "        s = s.replace('“', '\"')\n",
        "        s = s.replace('”', '\"')\n",
        "        s = s.replace('_', '')\n",
        "        s = re.sub('([。,;!?()\\\"~「」])', r' \\1 ', s) # keep punctuation\n",
        "    s = ' '.join(s.strip().split())\n",
        "    return s\n",
        "\n",
        "def len_s(s, lang):\n",
        "    if lang == 'zh':\n",
        "        return len(s)\n",
        "    return len(s.split())\n",
        "\n",
        "def clean_corpus(prefix, l1, l2, ratio=9, max_len=1000, min_len=1):\n",
        "    if Path(f'{prefix}.clean.{l1}').exists() and Path(f'{prefix}.clean.{l2}').exists():\n",
        "        print(f'{prefix}.clean.{l1} & {l2} exists. skipping clean.')\n",
        "        return\n",
        "    with open(f'{prefix}.{l1}', 'r') as l1_in_f:\n",
        "        with open(f'{prefix}.{l2}', 'r') as l2_in_f:\n",
        "            with open(f'{prefix}.clean.{l1}', 'w') as l1_out_f:\n",
        "                with open(f'{prefix}.clean.{l2}', 'w') as l2_out_f:\n",
        "                    for s1 in l1_in_f:\n",
        "                        s1 = s1.strip()\n",
        "                        s2 = l2_in_f.readline().strip()\n",
        "                        s1 = clean_s(s1, l1)\n",
        "                        s2 = clean_s(s2, l2)\n",
        "                        s1_len = len_s(s1, l1)\n",
        "                        s2_len = len_s(s2, l2)\n",
        "                        if min_len > 0: # remove short sentence\n",
        "                            if s1_len < min_len or s2_len < min_len:\n",
        "                                continue\n",
        "                        if max_len > 0: # remove long sentence\n",
        "                            if s1_len > max_len or s2_len > max_len:\n",
        "                                continue\n",
        "                        if ratio > 0: # remove by ratio of length\n",
        "                            if s1_len/s2_len > ratio or s2_len/s1_len > ratio:\n",
        "                                continue\n",
        "                        print(s1, file=l1_out_f)\n",
        "                        print(s2, file=l2_out_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "h_i8b1PRr9Nf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\DATA\\rawdata\\ted2020\\train_dev.raw.clean.zh & en exists. skipping clean.\n",
            "d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\DATA\\rawdata\\ted2020\\test.raw.clean.zh & en exists. skipping clean.\n"
          ]
        }
      ],
      "source": [
        "clean_corpus(data_prefix, src_lang, tgt_lang)\n",
        "clean_corpus(test_prefix, src_lang, tgt_lang, ratio=-1, min_len=-1, max_len=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gjT3XCy9r_rj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "非常謝謝你 , 克里斯 。 能有這個機會第二度踏上這個演講台\n",
            "真是一大榮幸 。 我非常感激 。\n",
            "這個研討會給我留下了極為深刻的印象 , 我想感謝大家對我之前演講的好評 。\n",
            "我是由衷的想這麼說 , 有部份原因是因為我真的有需要 !\n",
            "請你們設身處地為我想一想 !\n",
            "Thank you so much , Chris .\n",
            "And it's truly a great honor to have the opportunity to come to this stage twice ; I'm extremely grateful .\n",
            "I have been blown away by this conference , and I want to thank all of you for the many nice comments about what I had to say the other night .\n",
            "And I say that sincerely , partly because I need that .\n",
            "Put yourselves in my position .\n"
          ]
        }
      ],
      "source": [
        "!head \"{data_prefix+'.clean.'+src_lang}\" -n 5\n",
        "!head \"{data_prefix+'.clean.'+tgt_lang}\" -n 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKb4u67-sT_Z"
      },
      "source": [
        "## Split into train/valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AuFKeDz3sGHL"
      },
      "outputs": [],
      "source": [
        "valid_ratio = 0.01 # 3000~4000 would suffice\n",
        "train_ratio = 1 - valid_ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QR2NVldqsXyY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train/valid splits exists. skipping split.\n"
          ]
        }
      ],
      "source": [
        "if (prefix/f'train.clean.{src_lang}').exists() \\\n",
        "and (prefix/f'train.clean.{tgt_lang}').exists() \\\n",
        "and (prefix/f'valid.clean.{src_lang}').exists() \\\n",
        "and (prefix/f'valid.clean.{tgt_lang}').exists():\n",
        "    print(f'train/valid splits exists. skipping split.')\n",
        "else:\n",
        "    line_num = sum(1 for line in open(f'{data_prefix}.clean.{src_lang}'))\n",
        "    labels = list(range(line_num))\n",
        "    random.shuffle(labels)\n",
        "    for lang in [src_lang, tgt_lang]:\n",
        "        train_f = open(os.path.join(data_dir, dataset_name, f'train.clean.{lang}'), 'w')\n",
        "        valid_f = open(os.path.join(data_dir, dataset_name, f'valid.clean.{lang}'), 'w')\n",
        "        count = 0\n",
        "        for line in open(f'{data_prefix}.clean.{lang}', 'r'):\n",
        "            if labels[count]/line_num < train_ratio:\n",
        "                train_f.write(line)\n",
        "            else:\n",
        "                valid_f.write(line)\n",
        "            count += 1\n",
        "        train_f.close()\n",
        "        valid_f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1rwQysTsdJq"
      },
      "source": [
        "## Subword Units \n",
        "Out of vocabulary (OOV) has been a major problem in machine translation. This can be alleviated by using subword units.\n",
        "- We will use the [sentencepiece](#kudo-richardson-2018-sentencepiece) package\n",
        "- select 'unigram' or 'byte-pair encoding (BPE)' algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ecwllsa7sZRA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\DATA\\rawdata\\ted2020/spm8000.model exists. skipping spm_train.\n"
          ]
        }
      ],
      "source": [
        "import sentencepiece as spm\n",
        "vocab_size = 8000\n",
        "if (prefix/f'spm{vocab_size}.model').exists():\n",
        "    print(f'{prefix}/spm{vocab_size}.model exists. skipping spm_train.')\n",
        "else:\n",
        "    spm.SentencePieceTrainer.train(\n",
        "        input=','.join([f'{prefix}/train.clean.{src_lang}',\n",
        "                        f'{prefix}/valid.clean.{src_lang}',\n",
        "                        f'{prefix}/train.clean.{tgt_lang}',\n",
        "                        f'{prefix}/valid.clean.{tgt_lang}']),\n",
        "        model_prefix=prefix/f'spm{vocab_size}',\n",
        "        vocab_size=vocab_size,\n",
        "        character_coverage=1,\n",
        "        model_type='unigram', # 'bpe' works as well\n",
        "        input_sentence_size=1e6,\n",
        "        shuffle_input_sentence=True,\n",
        "        normalization_rule_name='nmt_nfkc_cf',\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lQPRNldqse_V"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\DATA\\rawdata\\ted2020\\train.zh exists. skipping spm_encode.\n",
            "d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\DATA\\rawdata\\ted2020\\train.en exists. skipping spm_encode.\n",
            "d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\DATA\\rawdata\\ted2020\\valid.zh exists. skipping spm_encode.\n",
            "d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\DATA\\rawdata\\ted2020\\valid.en exists. skipping spm_encode.\n",
            "d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\DATA\\rawdata\\ted2020\\test.zh exists. skipping spm_encode.\n",
            "d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\DATA\\rawdata\\ted2020\\test.en exists. skipping spm_encode.\n"
          ]
        }
      ],
      "source": [
        "spm_model = spm.SentencePieceProcessor(model_file=str(prefix/f'spm{vocab_size}.model'))\n",
        "in_tag = {\n",
        "    'train': 'train.clean',\n",
        "    'valid': 'valid.clean',\n",
        "    'test': 'test.raw.clean',\n",
        "}\n",
        "for split in ['train', 'valid', 'test']:\n",
        "    for lang in [src_lang, tgt_lang]:\n",
        "        out_path = prefix/f'{split}.{lang}'\n",
        "        if out_path.exists():\n",
        "            print(f\"{out_path} exists. skipping spm_encode.\")\n",
        "        else:\n",
        "            with open(prefix/f'{split}.{lang}', 'w') as out_f:\n",
        "                with open(prefix/f'{in_tag[split]}.{lang}', 'r') as in_f:\n",
        "                    for line in in_f:\n",
        "                        line = line.strip()\n",
        "                        tok = spm_model.encode(line, out_type=str)\n",
        "                        print(' '.join(tok), file=out_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4j6lXHjAsjXa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▁ 非常 謝 謝 你 ▁, ▁ 克 里 斯 ▁。 ▁ 能 有 這個 機會 第二 度 踏 上 這個 演講 台\n",
            "▁ 真 是 一 大 榮 幸 ▁。 ▁我 非常 感 激 ▁。\n",
            "▁這個 研 討 會 給我 留 下 了 極 為 深 刻 的 印 象 ▁, ▁我想 感 謝 大家 對我 之前 演講 的 好 評 ▁。\n",
            "▁我 是由 衷 的 想 這麼 說 ▁, ▁有 部份 原因 是因為 我 真的 有 需要 ▁!\n",
            "▁ 請 你們 設 身 處 地 為 我想 一 想 ▁!\n",
            "▁thank ▁you ▁so ▁much ▁, ▁chris ▁.\n",
            "▁and ▁it ' s ▁ t ru ly ▁a ▁great ▁ho n or ▁to ▁have ▁the ▁ op port un ity ▁to ▁come ▁to ▁this ▁st age ▁ t wi ce ▁; ▁i ' m ▁ex t re me ly ▁gr ate ful ▁.\n",
            "▁i ▁have ▁been ▁ bl ow n ▁away ▁by ▁this ▁con fer ence ▁, ▁and ▁i ▁want ▁to ▁thank ▁all ▁of ▁you ▁for ▁the ▁many ▁ ni ce ▁ com ment s ▁about ▁what ▁i ▁had ▁to ▁say ▁the ▁other ▁night ▁.\n",
            "▁and ▁i ▁say ▁that ▁since re ly ▁, ▁part ly ▁because ▁i ▁need ▁that ▁.\n",
            "▁put ▁your s el ve s ▁in ▁my ▁po s ition ▁.\n"
          ]
        }
      ],
      "source": [
        "!head \"{data_dir+'/'+dataset_name+'/train.'+src_lang}\" -n 5\n",
        "!head \"{data_dir+'/'+dataset_name+'/train.'+tgt_lang}\" -n 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59si_C0Wsms7"
      },
      "source": [
        "## Binarize the data with fairseq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "w-cHVLSpsknh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DATA\\data-bin\\ted2020 exists, will not overwrite!\n"
          ]
        }
      ],
      "source": [
        "binpath = Path('./DATA/data-bin', dataset_name)\n",
        "if binpath.exists():\n",
        "    print(binpath, \"exists, will not overwrite!\")\n",
        "else:\n",
        "    !python -m fairseq_cli.preprocess \\\n",
        "        --source-lang {src_lang}\\\n",
        "        --target-lang {tgt_lang}\\\n",
        "        --trainpref \"{prefix/'train'}\"\\\n",
        "        --validpref \"{prefix/'valid'}\"\\\n",
        "        --testpref \"{prefix/'test'}\"\\\n",
        "        --destdir {binpath}\\\n",
        "        --joined-dictionary\\\n",
        "        --workers 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szMuH1SWLPWA"
      },
      "source": [
        "# Configuration for experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5Luz3_tVLUxs"
      },
      "outputs": [],
      "source": [
        "config = Namespace(\n",
        "    datadir = \"./DATA/data-bin/ted2020_with_mono\",\n",
        "    savedir = \"./checkpoints/transformer-bt\", # \"./checkpoints/transformer\", # \"./checkpoints/rnn\",\n",
        "    source_lang = \"en\",\n",
        "    target_lang = \"zh\",\n",
        "    \n",
        "    # cpu threads when fetching & processing data.\n",
        "    num_workers=2,  \n",
        "    # batch size in terms of tokens. gradient accumulation increases the effective batchsize.\n",
        "    max_tokens=8192,\n",
        "    accum_steps=2,\n",
        "    \n",
        "    # the lr s calculated from Noam lr scheduler. you can tune the maximum lr by this factor.\n",
        "    lr_factor=2.,\n",
        "    lr_warmup=4000,\n",
        "    \n",
        "    # clipping gradient norm helps alleviate gradient exploding\n",
        "    clip_norm=1.0,\n",
        "    \n",
        "    # maximum epochs for training\n",
        "    max_epoch=100,\n",
        "    start_epoch=1,\n",
        "    \n",
        "    # early stop patience if no improvement\n",
        "    early_stop_patience=3,\n",
        "    \n",
        "    # beam size for beam search\n",
        "    beam=5, \n",
        "    # generate sequences of maximum length ax + b, where x is the source length\n",
        "    max_len_a=1.2, \n",
        "    max_len_b=10, \n",
        "    # when decoding, post process sentence by removing sentencepiece symbols and jieba tokenization.\n",
        "    post_process = \"sentencepiece\",\n",
        "    \n",
        "    # checkpoints\n",
        "    keep_last_epochs=5,\n",
        "    resume=None, # if resume from checkpoint name (under config.savedir)\n",
        "    \n",
        "    # logging\n",
        "    use_wandb=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjrJFvyQLg86"
      },
      "source": [
        "# Logging\n",
        "- logging package logs ordinary messages\n",
        "- wandb logs the loss, bleu, etc. in the training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-ZiMyDWALbDk"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=\"INFO\", # \"DEBUG\" \"WARNING\" \"ERROR\"\n",
        "    stream=sys.stdout,\n",
        ")\n",
        "proj = \"hw5.seq2seq\"\n",
        "logger = logging.getLogger(proj)\n",
        "if config.use_wandb:\n",
        "    import wandb\n",
        "    wandb.init(project=proj, name=Path(config.savedir).stem, config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNoSkK45Lmqc"
      },
      "source": [
        "# CUDA Environments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "oqrsbmcoLqMl"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 01:20:00 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-04-15 01:20:00 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 16.000 GB ; name = NVIDIA GeForce RTX 3080 Laptop GPU      \n",
            "2023-04-15 01:20:00 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n"
          ]
        }
      ],
      "source": [
        "cuda_env = utils.CudaEnvironment()\n",
        "utils.CudaEnvironment.pretty_print_cuda_env_list([cuda_env])\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbJuBIHLLt2D"
      },
      "source": [
        "# Dataloading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpG4EBRLwe_"
      },
      "source": [
        "## We borrow the TranslationTask from fairseq\n",
        "* used to load the binarized data created above\n",
        "* well-implemented data iterator (dataloader)\n",
        "* built-in task.source_dictionary and task.target_dictionary are also handy\n",
        "* well-implemented beach search decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3gSEy1uFLvVs"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 01:20:01 | INFO | fairseq.tasks.translation | [en] dictionary: 8000 types\n",
            "2023-04-15 01:20:01 | INFO | fairseq.tasks.translation | [zh] dictionary: 8000 types\n"
          ]
        }
      ],
      "source": [
        "from fairseq.tasks.translation import TranslationConfig, TranslationTask\n",
        "\n",
        "## setup task\n",
        "task_cfg = TranslationConfig(\n",
        "    data=config.datadir,\n",
        "    source_lang=config.source_lang,\n",
        "    target_lang=config.target_lang,\n",
        "    train_subset=\"train\",\n",
        "    required_seq_len_multiple=8,\n",
        "    dataset_impl=\"mmap\",\n",
        "    upsample_primary=1,\n",
        ")\n",
        "task = TranslationTask.setup_task(task_cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "mR7Bhov7L4IU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 01:20:01 | INFO | hw5.seq2seq | loading data for epoch 1\n",
            "2023-04-15 01:20:01 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: ./DATA/data-bin/ted2020_with_mono\\train.en-zh.en\n",
            "2023-04-15 01:20:01 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: ./DATA/data-bin/ted2020_with_mono\\train.en-zh.zh\n",
            "2023-04-15 01:20:01 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020_with_mono train en-zh 390041 examples\n",
            "2023-04-15 01:20:01 | INFO | fairseq.data.data_utils | loaded 781,713 examples from: ./DATA/data-bin/ted2020_with_mono\\train1.en-zh.en\n",
            "2023-04-15 01:20:01 | INFO | fairseq.data.data_utils | loaded 781,713 examples from: ./DATA/data-bin/ted2020_with_mono\\train1.en-zh.zh\n",
            "2023-04-15 01:20:01 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020_with_mono train1 en-zh 781713 examples\n",
            "2023-04-15 01:20:01 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: ./DATA/data-bin/ted2020_with_mono\\valid.en-zh.en\n",
            "2023-04-15 01:20:01 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: ./DATA/data-bin/ted2020_with_mono\\valid.en-zh.zh\n",
            "2023-04-15 01:20:01 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020_with_mono valid en-zh 3939 examples\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"loading data for epoch 1\")\n",
        "task.load_dataset(split=\"train\", epoch=1, combine=True) # combine if you have back-translation data.\n",
        "task.load_dataset(split=\"valid\", epoch=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "P0BCEm_9L6ig"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 1,\n",
            " 'source': tensor([  18,   14,    6, 2234,   60,   19,   80,    5,  256,   16,  405, 1407,\n",
            "        1706,    7,    2]),\n",
            " 'target': tensor([ 140,  690,   28,  270,   45,  151, 1142,  660,  606,  369, 3114, 2434,\n",
            "        1434,  192,    2])}\n",
            "\"Source: that's exactly what i do optical mind control .\"\n",
            "'Target: 這實在就是我所做的--光學操控思想'\n"
          ]
        }
      ],
      "source": [
        "sample = task.dataset(\"valid\")[1]\n",
        "pprint.pprint(sample)\n",
        "pprint.pprint(\n",
        "    \"Source: \" + \\\n",
        "    task.source_dictionary.string(\n",
        "        sample['source'],\n",
        "        config.post_process,\n",
        "    )\n",
        ")\n",
        "pprint.pprint(\n",
        "    \"Target: \" + \\\n",
        "    task.target_dictionary.string(\n",
        "        sample['target'],\n",
        "        config.post_process,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcfCVa2FMBSE"
      },
      "source": [
        "# Dataset iterator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBvc-B_6MKZM"
      },
      "source": [
        "* Controls every batch to contain no more than N tokens, which optimizes GPU memory efficiency\n",
        "* Shuffles the training set for every epoch\n",
        "* Ignore sentences exceeding maximum length\n",
        "* Pad all sentences in a batch to the same length, which enables parallel computing by GPU\n",
        "* Add eos and shift one token\n",
        "    - teacher forcing: to train the model to predict the next token based on prefix, we feed the right shifted target sequence as the decoder input.\n",
        "    - generally, prepending bos to the target would do the job (as shown below)\n",
        "![seq2seq](https://i.imgur.com/0zeDyuI.png)\n",
        "    - in fairseq however, this is done by moving the eos token to the begining. Empirically, this has the same effect. For instance:\n",
        "    ```\n",
        "    # output target (target) and Decoder input (prev_output_tokens): \n",
        "                   eos = 2\n",
        "                target = 419,  711,  238,  888,  792,   60,  968,    8,    2\n",
        "    prev_output_tokens = 2,  419,  711,  238,  888,  792,   60,  968,    8\n",
        "    ```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "OWFJFmCnMDXW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 01:20:03 | WARNING | fairseq.tasks.fairseq_task | 2,532 samples have invalid sizes and will be skipped, max_positions=(20, 20), first few sample ids=[29, 135, 2444, 3058, 682, 731, 235, 1558, 3383, 559]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'id': tensor([723]),\n",
              " 'nsentences': 1,\n",
              " 'ntokens': 18,\n",
              " 'net_input': {'src_tokens': tensor([[   1,    1,    1,    1,    1,   18,   26,   82,    8,  480,   15,  651,\n",
              "           1361,   38,    6,  176, 2696,   39,    5,  822,   92,  260,    7,    2]]),\n",
              "  'src_lengths': tensor([19]),\n",
              "  'prev_output_tokens': tensor([[   2,  140,  296,  318, 1560,   51,  568,  316,  225, 1952,  254,   78,\n",
              "            151, 2691,    9,  215, 1680,   10,    1,    1,    1,    1,    1,    1]])},\n",
              " 'target': tensor([[ 140,  296,  318, 1560,   51,  568,  316,  225, 1952,  254,   78,  151,\n",
              "          2691,    9,  215, 1680,   10,    2,    1,    1,    1,    1,    1,    1]])}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def load_data_iterator(task, split, epoch=1, max_tokens=4000, num_workers=1, cached=True):\n",
        "    batch_iterator = task.get_batch_iterator(\n",
        "        dataset=task.dataset(split),\n",
        "        max_tokens=max_tokens,\n",
        "        max_sentences=None,\n",
        "        max_positions=utils.resolve_max_positions(\n",
        "            task.max_positions(),\n",
        "            max_tokens,\n",
        "        ),\n",
        "        ignore_invalid_inputs=True,\n",
        "        seed=seed,\n",
        "        num_workers=num_workers,\n",
        "        epoch=epoch,\n",
        "        disable_iterator_cache=not cached,\n",
        "        # Set this to False to speed up. However, if set to False, changing max_tokens beyond \n",
        "        # first call of this method has no effect. \n",
        "    )\n",
        "    return batch_iterator\n",
        "\n",
        "demo_epoch_obj = load_data_iterator(task, \"valid\", epoch=1, max_tokens=20, num_workers=1, cached=False)\n",
        "demo_iter = demo_epoch_obj.next_epoch_itr(shuffle=True)\n",
        "sample = next(demo_iter)\n",
        "sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p86K-0g7Me4M"
      },
      "source": [
        "* each batch is a python dict, with string key and Tensor value. Contents are described below:\n",
        "```python\n",
        "batch = {\n",
        "    \"id\": id, # id for each example \n",
        "    \"nsentences\": len(samples), # batch size (sentences)\n",
        "    \"ntokens\": ntokens, # batch size (tokens)\n",
        "    \"net_input\": {\n",
        "        \"src_tokens\": src_tokens, # sequence in source language\n",
        "        \"src_lengths\": src_lengths, # sequence length of each example before padding\n",
        "        \"prev_output_tokens\": prev_output_tokens, # right shifted target, as mentioned above.\n",
        "    },\n",
        "    \"target\": target, # target sequence\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EyDBE5ZMkFZ"
      },
      "source": [
        "# Model Architecture\n",
        "* We again inherit fairseq's encoder, decoder and model, so that in the testing phase we can directly leverage fairseq's beam search decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Hzh74qLIMfW_"
      },
      "outputs": [],
      "source": [
        "from fairseq.models import (\n",
        "    FairseqEncoder, \n",
        "    FairseqIncrementalDecoder,\n",
        "    FairseqEncoderDecoderModel\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI46v1z7MotH"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn0wSeLLMrbc"
      },
      "source": [
        "- The Encoder is a RNN or Transformer Encoder. The following description is for RNN. For every input token, Encoder will generate a output vector and a hidden states vector, and the hidden states vector is passed on to the next step. In other words, the Encoder sequentially reads in the input sequence, and outputs a single vector at each timestep, then finally outputs the final hidden states, or content vector, at the last timestep.\n",
        "- Parameters:\n",
        "  - *args*\n",
        "      - encoder_embed_dim: the dimension of embeddings, this compresses the one-hot vector into fixed dimensions, which achieves dimension reduction\n",
        "      - encoder_ffn_embed_dim is the dimension of hidden states and output vectors\n",
        "      - encoder_layers is the number of layers for Encoder RNN\n",
        "      - dropout determines the probability of a neuron's activation being set to 0, in order to prevent overfitting. Generally this is applied in training, and removed in testing.\n",
        "  - *dictionary*: the dictionary provided by fairseq. it's used to obtain the padding index, and in turn the encoder padding mask. \n",
        "  - *embed_tokens*: an instance of token embeddings (nn.Embedding)\n",
        "\n",
        "- Inputs: \n",
        "    - *src_tokens*: integer sequence representing english e.g. 1, 28, 29, 205, 2 \n",
        "- Outputs: \n",
        "    - *outputs*: the output of RNN at each timestep, can be furthur processed by Attention\n",
        "    - *final_hiddens*: the hidden states of each timestep, will be passed to decoder for decoding\n",
        "    - *encoder_padding_mask*: this tells the decoder which position to ignore\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "WcX3W4iGMq-S"
      },
      "outputs": [],
      "source": [
        "class RNNEncoder(FairseqEncoder):\n",
        "    def __init__(self, args, dictionary, embed_tokens):\n",
        "        super().__init__(dictionary)\n",
        "        self.embed_tokens = embed_tokens\n",
        "        \n",
        "        self.embed_dim = args.encoder_embed_dim\n",
        "        self.hidden_dim = args.encoder_ffn_embed_dim\n",
        "        self.num_layers = args.encoder_layers\n",
        "        \n",
        "        self.dropout_in_module = nn.Dropout(args.dropout)\n",
        "        self.rnn = nn.GRU(\n",
        "            self.embed_dim, \n",
        "            self.hidden_dim, \n",
        "            self.num_layers, \n",
        "            dropout=args.dropout, \n",
        "            batch_first=False, \n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.dropout_out_module = nn.Dropout(args.dropout)\n",
        "        \n",
        "        self.padding_idx = dictionary.pad()\n",
        "        \n",
        "    def combine_bidir(self, outs, bsz: int):\n",
        "        out = outs.view(self.num_layers, 2, bsz, -1).transpose(1, 2).contiguous()\n",
        "        return out.view(self.num_layers, bsz, -1)\n",
        "\n",
        "    def forward(self, src_tokens, **unused):\n",
        "        bsz, seqlen = src_tokens.size()\n",
        "        \n",
        "        # get embeddings\n",
        "        x = self.embed_tokens(src_tokens)\n",
        "        x = self.dropout_in_module(x)\n",
        "\n",
        "        # B x T x C -> T x B x C\n",
        "        x = x.transpose(0, 1)\n",
        "        \n",
        "        # pass thru bidirectional RNN\n",
        "        h0 = x.new_zeros(2 * self.num_layers, bsz, self.hidden_dim)\n",
        "        x, final_hiddens = self.rnn(x, h0)\n",
        "        outputs = self.dropout_out_module(x)\n",
        "        # outputs = [sequence len, batch size, hid dim * directions]\n",
        "        # hidden =  [num_layers * directions, batch size  , hid dim]\n",
        "        \n",
        "        # Since Encoder is bidirectional, we need to concatenate the hidden states of two directions\n",
        "        final_hiddens = self.combine_bidir(final_hiddens, bsz)\n",
        "        # hidden =  [num_layers x batch x num_directions*hidden]\n",
        "        \n",
        "        encoder_padding_mask = src_tokens.eq(self.padding_idx).t()\n",
        "        return tuple(\n",
        "            (\n",
        "                outputs,  # seq_len x batch x hidden\n",
        "                final_hiddens,  # num_layers x batch x num_directions*hidden\n",
        "                encoder_padding_mask,  # seq_len x batch\n",
        "            )\n",
        "        )\n",
        "    \n",
        "    def reorder_encoder_out(self, encoder_out, new_order):\n",
        "        # This is used by fairseq's beam search. How and why is not particularly important here.\n",
        "        return tuple(\n",
        "            (\n",
        "                encoder_out[0].index_select(1, new_order),\n",
        "                encoder_out[1].index_select(1, new_order),\n",
        "                encoder_out[2].index_select(1, new_order),\n",
        "            )\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZlE_1JnMv56"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSFSKt_ZMzgh"
      },
      "source": [
        "- When the input sequence is long, \"content vector\" alone cannot accurately represent the whole sequence, attention mechanism can provide the Decoder more information.\n",
        "- According to the **Decoder embeddings** of the current timestep, match the **Encoder outputs** with decoder embeddings to determine correlation, and then sum the Encoder outputs weighted by the correlation as the input to **Decoder** RNN.\n",
        "- Common attention implementations use neural network / dot product as the correlation between **query** (decoder embeddings) and **key** (Encoder outputs), followed by **softmax**  to obtain a distribution, and finally **values** (Encoder outputs) is **weighted sum**-ed by said distribution.\n",
        "\n",
        "- Parameters:\n",
        "  - *input_embed_dim*: dimensionality of key, should be that of the vector in decoder to attend others\n",
        "  - *source_embed_dim*: dimensionality of query, should be that of the vector to be attended to (encoder outputs)\n",
        "  - *output_embed_dim*: dimensionality of value, should be that of the vector after attention, expected by the next layer\n",
        "\n",
        "- Inputs: \n",
        "    - *inputs*: is the key, the vector to attend to others\n",
        "    - *encoder_outputs*:  is the query/value, the vector to be attended to\n",
        "    - *encoder_padding_mask*: this tells the decoder which position to ignore\n",
        "- Outputs: \n",
        "    - *output*: the context vector after attention\n",
        "    - *attention score*: the attention distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1Atf_YuCMyyF"
      },
      "outputs": [],
      "source": [
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, input_embed_dim, source_embed_dim, output_embed_dim, bias=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_proj = nn.Linear(input_embed_dim, source_embed_dim, bias=bias)\n",
        "        self.output_proj = nn.Linear(\n",
        "            input_embed_dim + source_embed_dim, output_embed_dim, bias=bias\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs, encoder_outputs, encoder_padding_mask):\n",
        "        # inputs: T, B, dim\n",
        "        # encoder_outputs: S x B x dim\n",
        "        # padding mask:  S x B\n",
        "        \n",
        "        # convert all to batch first\n",
        "        inputs = inputs.transpose(1,0) # B, T, dim\n",
        "        encoder_outputs = encoder_outputs.transpose(1,0) # B, S, dim\n",
        "        encoder_padding_mask = encoder_padding_mask.transpose(1,0) # B, S\n",
        "        \n",
        "        # project to the dimensionality of encoder_outputs\n",
        "        x = self.input_proj(inputs)\n",
        "\n",
        "        # compute attention\n",
        "        # (B, T, dim) x (B, dim, S) = (B, T, S)\n",
        "        attn_scores = torch.bmm(x, encoder_outputs.transpose(1,2))\n",
        "\n",
        "        # cancel the attention at positions corresponding to padding\n",
        "        if encoder_padding_mask is not None:\n",
        "            # leveraging broadcast  B, S -> (B, 1, S)\n",
        "            encoder_padding_mask = encoder_padding_mask.unsqueeze(1)\n",
        "            attn_scores = (\n",
        "                attn_scores.float()\n",
        "                .masked_fill_(encoder_padding_mask, float(\"-inf\"))\n",
        "                .type_as(attn_scores)\n",
        "            )  # FP16 support: cast to float and back\n",
        "\n",
        "        # softmax on the dimension corresponding to source sequence\n",
        "        attn_scores = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        # shape (B, T, S) x (B, S, dim) = (B, T, dim) weighted sum\n",
        "        x = torch.bmm(attn_scores, encoder_outputs)\n",
        "\n",
        "        # (B, T, dim)\n",
        "        x = torch.cat((x, inputs), dim=-1)\n",
        "        x = torch.tanh(self.output_proj(x)) # concat + linear + tanh\n",
        "        \n",
        "        # restore shape (B, T, dim) -> (T, B, dim)\n",
        "        return x.transpose(1,0), attn_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doSCOA2gM7fK"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M8Vod2gNABR"
      },
      "source": [
        "* The hidden states of **Decoder** will be initialized by the final hidden states of **Encoder** (the content vector)\n",
        "* At the same time, **Decoder** will change its hidden states based on the input of the current timestep (the outputs of previous timesteps), and generates an output\n",
        "* Attention improves the performance\n",
        "* The seq2seq steps are implemented in decoder, so that later the Seq2Seq class can accept RNN and Transformer, without furthur modification.\n",
        "- Parameters:\n",
        "  - *args*\n",
        "      - decoder_embed_dim: is the dimensionality of the decoder embeddings, similar to encoder_embed_dim，\n",
        "      - decoder_ffn_embed_dim: is the dimensionality of the decoder RNN hidden states, similar to encoder_ffn_embed_dim\n",
        "      - decoder_layers: number of layers of RNN decoder\n",
        "      - share_decoder_input_output_embed: usually, the projection matrix of the decoder will share weights with the decoder input embeddings\n",
        "  - *dictionary*: the dictionary provided by fairseq\n",
        "  - *embed_tokens*: an instance of token embeddings (nn.Embedding)\n",
        "- Inputs: \n",
        "    - *prev_output_tokens*: integer sequence representing the right-shifted target e.g. 1, 28, 29, 205, 2 \n",
        "    - *encoder_out*: encoder's output.\n",
        "    - *incremental_state*: in order to speed up decoding during test time, we will save the hidden state of each timestep. see forward() for details.\n",
        "- Outputs: \n",
        "    - *outputs*: the logits (before softmax) output of decoder for each timesteps\n",
        "    - *extra*: unsused"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "QfvgqHYDM6Lp"
      },
      "outputs": [],
      "source": [
        "class RNNDecoder(FairseqIncrementalDecoder):\n",
        "    def __init__(self, args, dictionary, embed_tokens):\n",
        "        super().__init__(dictionary)\n",
        "        self.embed_tokens = embed_tokens\n",
        "        \n",
        "        assert args.decoder_layers == args.encoder_layers, f\"\"\"seq2seq rnn requires that encoder \n",
        "        and decoder have same layers of rnn. got: {args.encoder_layers, args.decoder_layers}\"\"\"\n",
        "        assert args.decoder_ffn_embed_dim == args.encoder_ffn_embed_dim*2, f\"\"\"seq2seq-rnn requires \n",
        "        that decoder hidden to be 2*encoder hidden dim. got: {args.decoder_ffn_embed_dim, args.encoder_ffn_embed_dim*2}\"\"\"\n",
        "        \n",
        "        self.embed_dim = args.decoder_embed_dim\n",
        "        self.hidden_dim = args.decoder_ffn_embed_dim\n",
        "        self.num_layers = args.decoder_layers\n",
        "        \n",
        "        \n",
        "        self.dropout_in_module = nn.Dropout(args.dropout)\n",
        "        self.rnn = nn.GRU(\n",
        "            self.embed_dim, \n",
        "            self.hidden_dim, \n",
        "            self.num_layers, \n",
        "            dropout=args.dropout, \n",
        "            batch_first=False, \n",
        "            bidirectional=False\n",
        "        )\n",
        "        self.attention = AttentionLayer(\n",
        "            self.embed_dim, self.hidden_dim, self.embed_dim, bias=False\n",
        "        ) \n",
        "        # self.attention = None\n",
        "        self.dropout_out_module = nn.Dropout(args.dropout)\n",
        "        \n",
        "        if self.hidden_dim != self.embed_dim:\n",
        "            self.project_out_dim = nn.Linear(self.hidden_dim, self.embed_dim)\n",
        "        else:\n",
        "            self.project_out_dim = None\n",
        "        \n",
        "        if args.share_decoder_input_output_embed:\n",
        "            self.output_projection = nn.Linear(\n",
        "                self.embed_tokens.weight.shape[1],\n",
        "                self.embed_tokens.weight.shape[0],\n",
        "                bias=False,\n",
        "            )\n",
        "            self.output_projection.weight = self.embed_tokens.weight\n",
        "        else:\n",
        "            self.output_projection = nn.Linear(\n",
        "                self.output_embed_dim, len(dictionary), bias=False\n",
        "            )\n",
        "            nn.init.normal_(\n",
        "                self.output_projection.weight, mean=0, std=self.output_embed_dim ** -0.5\n",
        "            )\n",
        "        \n",
        "    def forward(self, prev_output_tokens, encoder_out, incremental_state=None, **unused):\n",
        "        # extract the outputs from encoder\n",
        "        encoder_outputs, encoder_hiddens, encoder_padding_mask = encoder_out\n",
        "        # outputs:          seq_len x batch x num_directions*hidden\n",
        "        # encoder_hiddens:  num_layers x batch x num_directions*encoder_hidden\n",
        "        # padding_mask:     seq_len x batch\n",
        "        \n",
        "        if incremental_state is not None and len(incremental_state) > 0:\n",
        "            # if the information from last timestep is retained, we can continue from there instead of starting from bos\n",
        "            prev_output_tokens = prev_output_tokens[:, -1:]\n",
        "            cache_state = self.get_incremental_state(incremental_state, \"cached_state\")\n",
        "            prev_hiddens = cache_state[\"prev_hiddens\"]\n",
        "        else:\n",
        "            # incremental state does not exist, either this is training time, or the first timestep of test time\n",
        "            # prepare for seq2seq: pass the encoder_hidden to the decoder hidden states\n",
        "            prev_hiddens = encoder_hiddens\n",
        "        \n",
        "        bsz, seqlen = prev_output_tokens.size()\n",
        "        \n",
        "        # embed tokens\n",
        "        x = self.embed_tokens(prev_output_tokens)\n",
        "        x = self.dropout_in_module(x)\n",
        "\n",
        "        # B x T x C -> T x B x C\n",
        "        x = x.transpose(0, 1)\n",
        "                \n",
        "        # decoder-to-encoder attention\n",
        "        if self.attention is not None:\n",
        "            x, attn = self.attention(x, encoder_outputs, encoder_padding_mask)\n",
        "                        \n",
        "        # pass thru unidirectional RNN\n",
        "        x, final_hiddens = self.rnn(x, prev_hiddens)\n",
        "        # outputs = [sequence len, batch size, hid dim]\n",
        "        # hidden =  [num_layers * directions, batch size  , hid dim]\n",
        "        x = self.dropout_out_module(x)\n",
        "                \n",
        "        # project to embedding size (if hidden differs from embed size, and share_embedding is True, \n",
        "        # we need to do an extra projection)\n",
        "        if self.project_out_dim != None:\n",
        "            x = self.project_out_dim(x)\n",
        "        \n",
        "        # project to vocab size\n",
        "        x = self.output_projection(x)\n",
        "        \n",
        "        # T x B x C -> B x T x C\n",
        "        x = x.transpose(1, 0)\n",
        "        \n",
        "        # if incremental, record the hidden states of current timestep, which will be restored in the next timestep\n",
        "        cache_state = {\n",
        "            \"prev_hiddens\": final_hiddens,\n",
        "        }\n",
        "        self.set_incremental_state(incremental_state, \"cached_state\", cache_state)\n",
        "        \n",
        "        return x, None\n",
        "    \n",
        "    def reorder_incremental_state(\n",
        "        self,\n",
        "        incremental_state,\n",
        "        new_order,\n",
        "    ):\n",
        "        # This is used by fairseq's beam search. How and why is not particularly important here.\n",
        "        cache_state = self.get_incremental_state(incremental_state, \"cached_state\")\n",
        "        prev_hiddens = cache_state[\"prev_hiddens\"]\n",
        "        prev_hiddens = [p.index_select(0, new_order) for p in prev_hiddens]\n",
        "        cache_state = {\n",
        "            \"prev_hiddens\": torch.stack(prev_hiddens),\n",
        "        }\n",
        "        self.set_incremental_state(incremental_state, \"cached_state\", cache_state)\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDAPmxjRNEEL"
      },
      "source": [
        "## Seq2Seq\n",
        "- Composed of **Encoder** and **Decoder**\n",
        "- Recieves inputs and pass to **Encoder** \n",
        "- Pass the outputs from **Encoder** to **Decoder**\n",
        "- **Decoder** will decode according to outputs of previous timesteps as well as **Encoder** outputs  \n",
        "- Once done decoding, return the **Decoder** outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "oRwKdLa0NEU6"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(FairseqEncoderDecoderModel):\n",
        "    def __init__(self, args, encoder, decoder):\n",
        "        super().__init__(encoder, decoder)\n",
        "        self.args = args\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        src_tokens,\n",
        "        src_lengths,\n",
        "        prev_output_tokens,\n",
        "        return_all_hiddens: bool = True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Run the forward pass for an encoder-decoder model.\n",
        "        \"\"\"\n",
        "        encoder_out = self.encoder(\n",
        "            src_tokens, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens\n",
        "        )\n",
        "        logits, extra = self.decoder(\n",
        "            prev_output_tokens,\n",
        "            encoder_out=encoder_out,\n",
        "            src_lengths=src_lengths,\n",
        "            return_all_hiddens=return_all_hiddens,\n",
        "        )\n",
        "        return logits, extra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu3C2JfqNHzk"
      },
      "source": [
        "# Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "nyI9FOx-NJ2m"
      },
      "outputs": [],
      "source": [
        "# # HINT: transformer architecture\n",
        "from fairseq.models.transformer import (\n",
        "    TransformerEncoder, \n",
        "    TransformerDecoder,\n",
        ")\n",
        "\n",
        "def build_model(args, task):\n",
        "    \"\"\" build a model instance based on hyperparameters \"\"\"\n",
        "    src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n",
        "\n",
        "    # token embeddings\n",
        "    encoder_embed_tokens = nn.Embedding(len(src_dict), args.encoder_embed_dim, src_dict.pad())\n",
        "    decoder_embed_tokens = nn.Embedding(len(tgt_dict), args.decoder_embed_dim, tgt_dict.pad())\n",
        "    \n",
        "    # encoder decoder\n",
        "    # HINT: TODO: switch to TransformerEncoder & TransformerDecoder\n",
        "    # encoder = RNNEncoder(args, src_dict, encoder_embed_tokens)\n",
        "    # decoder = RNNDecoder(args, tgt_dict, decoder_embed_tokens)\n",
        "    encoder = TransformerEncoder(args, src_dict, encoder_embed_tokens)\n",
        "    decoder = TransformerDecoder(args, tgt_dict, decoder_embed_tokens)\n",
        "\n",
        "    # sequence to sequence model\n",
        "    model = Seq2Seq(args, encoder, decoder)\n",
        "    \n",
        "    # initialization for seq2seq model is important, requires extra handling\n",
        "    def init_params(module):\n",
        "        from fairseq.modules import MultiheadAttention\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        if isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        if isinstance(module, MultiheadAttention):\n",
        "            module.q_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            module.k_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            module.v_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "        if isinstance(module, nn.RNNBase):\n",
        "            for name, param in module.named_parameters():\n",
        "                if \"weight\" in name or \"bias\" in name:\n",
        "                    param.data.uniform_(-0.1, 0.1)\n",
        "            \n",
        "    # weight initialization\n",
        "    model.apply(init_params)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce5n4eS7NQNy"
      },
      "source": [
        "## Architecture Related Configuration\n",
        "\n",
        "For strong baseline, please refer to the hyperparameters for *transformer-base* in Table 3 in [Attention is all you need](#vaswani2017)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Cyn30VoGNT6N"
      },
      "outputs": [],
      "source": [
        "arch_args = Namespace(\n",
        "    encoder_embed_dim=256,\n",
        "    encoder_ffn_embed_dim=512,\n",
        "    encoder_layers=4, # 1,\n",
        "    decoder_embed_dim=256,\n",
        "    decoder_ffn_embed_dim=1024,\n",
        "    decoder_layers=4, # 1,\n",
        "    share_decoder_input_output_embed=True,\n",
        "    dropout=0.3,\n",
        ")\n",
        "\n",
        "# HINT: these patches on parameters for Transformer\n",
        "def add_transformer_args(args):\n",
        "    args.encoder_attention_heads=4\n",
        "    args.encoder_normalize_before=True\n",
        "    \n",
        "    args.decoder_attention_heads=4\n",
        "    args.decoder_normalize_before=True\n",
        "    \n",
        "    args.activation_fn=\"relu\"\n",
        "    args.max_source_positions=1024\n",
        "    args.max_target_positions=1024\n",
        "    \n",
        "    # patches on default parameters for Transformer (those not set above)\n",
        "    from fairseq.models.transformer import base_architecture\n",
        "    base_architecture(arch_args)\n",
        "\n",
        "add_transformer_args(arch_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Nbb76QLCNZZZ"
      },
      "outputs": [],
      "source": [
        "if config.use_wandb:\n",
        "    wandb.config.update(vars(arch_args))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "7ZWfxsCDNatH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 01:20:10 | INFO | hw5.seq2seq | Seq2Seq(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(8000, 256, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(8000, 256, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (output_projection): Linear(in_features=256, out_features=8000, bias=False)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = build_model(arch_args, task)\n",
        "logger.info(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHll7GRNNdqc"
      },
      "source": [
        "# Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUB9f1WCNgMH"
      },
      "source": [
        "## Loss: Label Smoothing Regularization\n",
        "* let the model learn to generate less concentrated distribution, and prevent over-confidence\n",
        "* sometimes the ground truth may not be the only answer. thus, when calculating loss, we reserve some probability for incorrect labels\n",
        "* avoids overfitting\n",
        "\n",
        "code [source](https://fairseq.readthedocs.io/en/latest/_modules/fairseq/criterions/label_smoothed_cross_entropy.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "IgspdJn0NdYF"
      },
      "outputs": [],
      "source": [
        "class LabelSmoothedCrossEntropyCriterion(nn.Module):\n",
        "    def __init__(self, smoothing, ignore_index=None, reduce=True):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.ignore_index = ignore_index\n",
        "        self.reduce = reduce\n",
        "    \n",
        "    def forward(self, lprobs, target):\n",
        "        if target.dim() == lprobs.dim() - 1:\n",
        "            target = target.unsqueeze(-1)\n",
        "        # nll: Negative log likelihood，the cross-entropy when target is one-hot. following line is same as F.nll_loss\n",
        "        nll_loss = -lprobs.gather(dim=-1, index=target)\n",
        "        #  reserve some probability for other labels. thus when calculating cross-entropy, \n",
        "        # equivalent to summing the log probs of all labels\n",
        "        smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
        "        if self.ignore_index is not None:\n",
        "            pad_mask = target.eq(self.ignore_index)\n",
        "            nll_loss.masked_fill_(pad_mask, 0.0)\n",
        "            smooth_loss.masked_fill_(pad_mask, 0.0)\n",
        "        else:\n",
        "            nll_loss = nll_loss.squeeze(-1)\n",
        "            smooth_loss = smooth_loss.squeeze(-1)\n",
        "        if self.reduce:\n",
        "            nll_loss = nll_loss.sum()\n",
        "            smooth_loss = smooth_loss.sum()\n",
        "        # when calculating cross-entropy, add the loss of other labels\n",
        "        eps_i = self.smoothing / lprobs.size(-1)\n",
        "        loss = (1.0 - self.smoothing) * nll_loss + eps_i * smooth_loss\n",
        "        return loss\n",
        "\n",
        "# generally, 0.1 is good enough\n",
        "criterion = LabelSmoothedCrossEntropyCriterion(\n",
        "    smoothing=0.1,\n",
        "    ignore_index=task.target_dictionary.pad(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRalDto2NkJJ"
      },
      "source": [
        "## Optimizer: Adam + lr scheduling\n",
        "Inverse square root scheduling is important to the stability when training Transformer. It's later used on RNN as well.\n",
        "Update the learning rate according to the following equation. Linearly increase the first stage, then decay proportionally to the inverse square root of timestep.\n",
        "$$lrate = d_{\\text{model}}^{-0.5}\\cdot\\min({step\\_num}^{-0.5},{step\\_num}\\cdot{warmup\\_steps}^{-1.5})$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "sS7tQj1ROBYm"
      },
      "outputs": [],
      "source": [
        "def get_rate(d_model, step_num, warmup_step):\n",
        "    # TODO: Change lr from constant to the equation shown above\n",
        "    # lr = 0.001\n",
        "    lr = d_model ** (-0.5) * min(step_num ** (-0.5), step_num * warmup_step ** (-1.5)) \n",
        "    return lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "J8hoAjHPNkh3"
      },
      "outputs": [],
      "source": [
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "    \n",
        "    @property\n",
        "    def param_groups(self):\n",
        "        return self.optimizer.param_groups\n",
        "        \n",
        "    def multiply_grads(self, c):\n",
        "        \"\"\"Multiplies grads by a constant *c*.\"\"\"                \n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is not None:\n",
        "                    p.grad.data.mul_(c)\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return 0 if not step else self.factor * get_rate(self.model_size, step, self.warmup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFJlkOMONsc6"
      },
      "source": [
        "## Scheduling Visualized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "A135fwPCNrQs"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "optimizer = NoamOpt(\n",
        "    model_size=arch_args.encoder_embed_dim, \n",
        "    factor=config.lr_factor, \n",
        "    warmup=config.lr_warmup, \n",
        "    optimizer=torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0001))\n",
        "plt.plot(np.arange(1, 100000), [optimizer.rate(i) for i in range(1, 100000)])\n",
        "plt.legend([f\"{optimizer.model_size}:{optimizer.warmup}\"])\n",
        "None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOR0g-cVO5ZO"
      },
      "source": [
        "# Training Procedure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-0ZjbK3O8Iv"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "foal3xM1O404"
      },
      "outputs": [],
      "source": [
        "from fairseq.data import iterators\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "def train_one_epoch(epoch_itr, model, task, criterion, optimizer, accum_steps=1):\n",
        "    itr = epoch_itr.next_epoch_itr(shuffle=True)\n",
        "    itr = iterators.GroupedIterator(itr, accum_steps) # gradient accumulation: update every accum_steps samples\n",
        "    \n",
        "    stats = {\"loss\": [], \"gnorm\": []}\n",
        "    scaler = GradScaler() # automatic mixed precision (amp) \n",
        "    \n",
        "    model.train()\n",
        "    progress = tqdm.tqdm(itr, desc=f\"train epoch {epoch_itr.epoch}\", leave=False)\n",
        "    for samples in progress:\n",
        "        model.zero_grad()\n",
        "        accum_loss = 0\n",
        "        sample_size = 0\n",
        "        # gradient accumulation: update every accum_steps samples\n",
        "        for i, sample in enumerate(samples):\n",
        "            if i == 1:\n",
        "                # emptying the CUDA cache after the first step can reduce the chance of OOM\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            sample = utils.move_to_cuda(sample, device=device)\n",
        "            target = sample[\"target\"]\n",
        "            sample_size_i = sample[\"ntokens\"]\n",
        "            sample_size += sample_size_i\n",
        "            \n",
        "            # mixed precision training\n",
        "            with autocast():\n",
        "                net_output = model.forward(**sample[\"net_input\"])\n",
        "                lprobs = F.log_softmax(net_output[0], -1)            \n",
        "                loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1))\n",
        "                \n",
        "                # logging\n",
        "                accum_loss += loss.item()\n",
        "                # back-prop\n",
        "                scaler.scale(loss).backward()                \n",
        "        \n",
        "        scaler.unscale_(optimizer)\n",
        "        optimizer.multiply_grads(1 / (sample_size or 1.0)) # (sample_size or 1.0) handles the case of a zero gradient\n",
        "        gnorm = nn.utils.clip_grad_norm_(model.parameters(), config.clip_norm) # grad norm clipping prevents gradient exploding\n",
        "        \n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "        # logging\n",
        "        loss_print = accum_loss/sample_size\n",
        "        stats[\"loss\"].append(loss_print)\n",
        "        stats[\"gnorm\"].append(gnorm.item())\n",
        "        progress.set_postfix(loss=loss_print)\n",
        "        if config.use_wandb:\n",
        "            wandb.log({\n",
        "                \"train/loss\": loss_print,\n",
        "                \"train/grad_norm\": gnorm.item(),\n",
        "                \"train/lr\": optimizer.rate(),\n",
        "                \"train/sample_size\": sample_size,\n",
        "            })\n",
        "        \n",
        "    loss_print = np.mean(stats[\"loss\"])\n",
        "    logger.info(f\"training loss: {loss_print:.4f}\")\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt1lX3DRO_yU"
      },
      "source": [
        "## Validation & Inference\n",
        "To prevent overfitting, validation is required every epoch to validate the performance on unseen data.\n",
        "- the procedure is essensially same as training, with the addition of inference step\n",
        "- after validation we can save the model weights\n",
        "\n",
        "Validation loss alone cannot describe the actual performance of the model\n",
        "- Directly produce translation hypotheses based on current model, then calculate BLEU with the reference translation\n",
        "- We can also manually examine the hypotheses' quality\n",
        "- We use fairseq's sequence generator for beam search to generate translation hypotheses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "2og80HYQPAKq"
      },
      "outputs": [],
      "source": [
        "# fairseq's beam search generator\n",
        "# given model and input seqeunce, produce translation hypotheses by beam search\n",
        "sequence_generator = task.build_generator([model], config)\n",
        "\n",
        "def decode(toks, dictionary):\n",
        "    # convert from Tensor to human readable sentence\n",
        "    s = dictionary.string(\n",
        "        toks.int().cpu(),\n",
        "        config.post_process,\n",
        "    )\n",
        "    return s if s else \"<unk>\"\n",
        "\n",
        "def inference_step(sample, model):\n",
        "    gen_out = sequence_generator.generate([model], sample)\n",
        "    srcs = []\n",
        "    hyps = []\n",
        "    refs = []\n",
        "    for i in range(len(gen_out)):\n",
        "        # for each sample, collect the input, hypothesis and reference, later be used to calculate BLEU\n",
        "        srcs.append(decode(\n",
        "            utils.strip_pad(sample[\"net_input\"][\"src_tokens\"][i], task.source_dictionary.pad()), \n",
        "            task.source_dictionary,\n",
        "        ))\n",
        "        hyps.append(decode(\n",
        "            gen_out[i][0][\"tokens\"], # 0 indicates using the top hypothesis in beam\n",
        "            task.target_dictionary,\n",
        "        ))\n",
        "        refs.append(decode(\n",
        "            utils.strip_pad(sample[\"target\"][i], task.target_dictionary.pad()), \n",
        "            task.target_dictionary,\n",
        "        ))\n",
        "    return srcs, hyps, refs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "y1o7LeDkPDsd"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import sacrebleu\n",
        "\n",
        "def validate(model, task, criterion, log_to_wandb=True):\n",
        "    logger.info('begin validation')\n",
        "    itr = load_data_iterator(task, \"valid\", 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
        "    \n",
        "    stats = {\"loss\":[], \"bleu\": 0, \"srcs\":[], \"hyps\":[], \"refs\":[]}\n",
        "    srcs = []\n",
        "    hyps = []\n",
        "    refs = []\n",
        "    \n",
        "    model.eval()\n",
        "    progress = tqdm.tqdm(itr, desc=f\"validation\", leave=False)\n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(progress):\n",
        "            # validation loss\n",
        "            sample = utils.move_to_cuda(sample, device=device)\n",
        "            net_output = model.forward(**sample[\"net_input\"])\n",
        "\n",
        "            lprobs = F.log_softmax(net_output[0], -1)\n",
        "            target = sample[\"target\"]\n",
        "            sample_size = sample[\"ntokens\"]\n",
        "            loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1)) / sample_size\n",
        "            progress.set_postfix(valid_loss=loss.item())\n",
        "            stats[\"loss\"].append(loss)\n",
        "            \n",
        "            # do inference\n",
        "            s, h, r = inference_step(sample, model)\n",
        "            srcs.extend(s)\n",
        "            hyps.extend(h)\n",
        "            refs.extend(r)\n",
        "            \n",
        "    tok = 'zh' if task.cfg.target_lang == 'zh' else '13a'\n",
        "    stats[\"loss\"] = torch.stack(stats[\"loss\"]).mean().item()\n",
        "    stats[\"bleu\"] = sacrebleu.corpus_bleu(hyps, [refs], tokenize=tok) # 計算BLEU score\n",
        "    stats[\"srcs\"] = srcs\n",
        "    stats[\"hyps\"] = hyps\n",
        "    stats[\"refs\"] = refs\n",
        "    \n",
        "    if config.use_wandb and log_to_wandb:\n",
        "        wandb.log({\n",
        "            \"valid/loss\": stats[\"loss\"],\n",
        "            \"valid/bleu\": stats[\"bleu\"].score,\n",
        "        }, commit=False)\n",
        "    \n",
        "    showid = np.random.randint(len(hyps))\n",
        "    logger.info(\"example source: \" + srcs[showid])\n",
        "    logger.info(\"example hypothesis: \" + hyps[showid])\n",
        "    logger.info(\"example reference: \" + refs[showid])\n",
        "    \n",
        "    # show bleu results\n",
        "    logger.info(f\"validation loss:\\t{stats['loss']:.4f}\")\n",
        "    logger.info(stats[\"bleu\"].format())\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sRF6nd4PGEE"
      },
      "source": [
        "# Save and Load Model Weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "edBuLlkuPGr9"
      },
      "outputs": [],
      "source": [
        "def validate_and_save(model, task, criterion, optimizer, epoch, save=True):   \n",
        "    stats = validate(model, task, criterion)\n",
        "    bleu = stats['bleu']\n",
        "    loss = stats['loss']\n",
        "    if save:\n",
        "        # save epoch checkpoints\n",
        "        savedir = Path(config.savedir).absolute()\n",
        "        savedir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        check = {\n",
        "            \"model\": model.state_dict(),\n",
        "            \"stats\": {\"bleu\": bleu.score, \"loss\": loss},\n",
        "            \"optim\": {\"step\": optimizer._step}\n",
        "        }\n",
        "        torch.save(check, savedir/f\"checkpoint{epoch}.pt\")\n",
        "        shutil.copy(savedir/f\"checkpoint{epoch}.pt\", savedir/f\"checkpoint_last.pt\")\n",
        "        logger.info(f\"saved epoch checkpoint: {savedir}/checkpoint{epoch}.pt\")\n",
        "    \n",
        "        # save epoch samples\n",
        "        with open(savedir/f\"samples{epoch}.{config.source_lang}-{config.target_lang}.txt\", \"w\") as f:\n",
        "            for s, h in zip(stats[\"srcs\"], stats[\"hyps\"]):\n",
        "                f.write(f\"{s}\\t{h}\\n\")\n",
        "\n",
        "        # get best valid bleu    \n",
        "        if getattr(validate_and_save, \"best_bleu\", 0) < bleu.score:\n",
        "            validate_and_save.best_bleu = bleu.score\n",
        "            torch.save(check, savedir/f\"checkpoint_best.pt\")\n",
        "            \n",
        "        del_file = savedir / f\"checkpoint{epoch - config.keep_last_epochs}.pt\"\n",
        "        if del_file.exists():\n",
        "            del_file.unlink()\n",
        "    \n",
        "    return stats\n",
        "\n",
        "def try_load_checkpoint(model, optimizer=None, name=None):\n",
        "    name = name if name else \"checkpoint_last.pt\"\n",
        "    checkpath = Path(config.savedir)/name\n",
        "    if checkpath.exists():\n",
        "        check = torch.load(checkpath)\n",
        "        model.load_state_dict(check[\"model\"])\n",
        "        stats = check[\"stats\"]\n",
        "        step = \"unknown\"\n",
        "        if optimizer != None:\n",
        "            optimizer._step = step = check[\"optim\"][\"step\"]\n",
        "        logger.info(f\"loaded checkpoint {checkpath}: step={step} loss={stats['loss']} bleu={stats['bleu']}\")\n",
        "    else:\n",
        "        logger.info(f\"no checkpoints found at {checkpath}!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyIFpibfPJ5u"
      },
      "source": [
        "# Main\n",
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "hu7RZbCUPKQr"
      },
      "outputs": [],
      "source": [
        "model = model.to(device=device)\n",
        "criterion = criterion.to(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "5xxlJxU2PeAo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 01:20:17 | INFO | hw5.seq2seq | task: TranslationTask\n",
            "2023-04-15 01:20:17 | INFO | hw5.seq2seq | encoder: TransformerEncoder\n",
            "2023-04-15 01:20:17 | INFO | hw5.seq2seq | decoder: TransformerDecoder\n",
            "2023-04-15 01:20:17 | INFO | hw5.seq2seq | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2023-04-15 01:20:17 | INFO | hw5.seq2seq | optimizer: NoamOpt\n",
            "2023-04-15 01:20:17 | INFO | hw5.seq2seq | num. model params: 12,523,520 (num. trained: 12,523,520)\n",
            "2023-04-15 01:20:17 | INFO | hw5.seq2seq | max tokens per batch = 8192, accumulate steps = 2\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"task: {}\".format(task.__class__.__name__))\n",
        "logger.info(\"encoder: {}\".format(model.encoder.__class__.__name__))\n",
        "logger.info(\"decoder: {}\".format(model.decoder.__class__.__name__))\n",
        "logger.info(\"criterion: {}\".format(criterion.__class__.__name__))\n",
        "logger.info(\"optimizer: {}\".format(optimizer.__class__.__name__))\n",
        "logger.info(\n",
        "    \"num. model params: {:,} (num. trained: {:,})\".format(\n",
        "        sum(p.numel() for p in model.parameters()),\n",
        "        sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
        "    )\n",
        ")\n",
        "logger.info(f\"max tokens per batch = {config.max_tokens}, accumulate steps = {config.accum_steps}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "MSPRqpQUPfaX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 01:20:19 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[326674]\n",
            "2023-04-15 01:20:19 | INFO | hw5.seq2seq | no checkpoints found at checkpoints\\transformer-bt\\checkpoint_last.pt!\n",
            "2023-04-15 01:20:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1851\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "65fc73dc83e34d37a0b58e214c5ea47c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 1:   0%|          | 0/1851 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 01:26:53 | INFO | hw5.seq2seq | training loss: 5.9368\n",
            "2023-04-15 01:26:53 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f69d006c4d754428875897b80f479b1f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 01:27:20 | INFO | hw5.seq2seq | example source: so low income here , high income there .\n",
            "2023-04-15 01:27:20 | INFO | hw5.seq2seq | example hypothesis: 所以低入這裡 , 在高速中 ,\n",
            "2023-04-15 01:27:20 | INFO | hw5.seq2seq | example reference: 所以這裡是收入低的 , 這裡是收入高的\n",
            "2023-04-15 01:27:20 | INFO | hw5.seq2seq | validation loss:\t4.7905\n",
            "2023-04-15 01:27:20 | INFO | hw5.seq2seq | BLEU = 11.01 46.6/20.4/9.7/4.9 (BP = 0.756 ratio = 0.781 hyp_len = 87356 ref_len = 111811)\n",
            "2023-04-15 01:27:20 | INFO | hw5.seq2seq | saved epoch checkpoint: d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\checkpoints\\transformer-bt/checkpoint1.pt\n",
            "2023-04-15 01:27:20 | INFO | hw5.seq2seq | end of epoch 1\n",
            "2023-04-15 01:27:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1851\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72298af22d1646beae132709cb06b9b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 2:   0%|          | 0/1851 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 01:33:55 | INFO | hw5.seq2seq | training loss: 4.2255\n",
            "2023-04-15 01:33:55 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c07e8822cc8044d69429e970bd70802e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 01:34:24 | INFO | hw5.seq2seq | example source: you should have started with z .\n",
            "2023-04-15 01:34:24 | INFO | hw5.seq2seq | example hypothesis: 你應該從z開始 。\n",
            "2023-04-15 01:34:24 | INFO | hw5.seq2seq | example reference: 要從z開始讀 。\n",
            "2023-04-15 01:34:24 | INFO | hw5.seq2seq | validation loss:\t4.2190\n",
            "2023-04-15 01:34:24 | INFO | hw5.seq2seq | BLEU = 17.48 50.0/24.7/12.9/7.1 (BP = 0.953 ratio = 0.954 hyp_len = 106657 ref_len = 111811)\n",
            "2023-04-15 01:34:25 | INFO | hw5.seq2seq | saved epoch checkpoint: d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\checkpoints\\transformer-bt/checkpoint2.pt\n",
            "2023-04-15 01:34:25 | INFO | hw5.seq2seq | end of epoch 2\n",
            "2023-04-15 01:34:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1851\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "804771373c544c01a0636ba484a71905",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 3:   0%|          | 0/1851 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 01:41:02 | INFO | hw5.seq2seq | training loss: 3.8211\n",
            "2023-04-15 01:41:02 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aea58f301dd24714a3c838008583265e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 01:41:25 | INFO | hw5.seq2seq | example source: grouper ?\n",
            "2023-04-15 01:41:25 | INFO | hw5.seq2seq | example hypothesis: 小組嗎 ?\n",
            "2023-04-15 01:41:25 | INFO | hw5.seq2seq | example reference: 石斑魚 ?\n",
            "2023-04-15 01:41:25 | INFO | hw5.seq2seq | validation loss:\t3.9672\n",
            "2023-04-15 01:41:25 | INFO | hw5.seq2seq | BLEU = 20.02 53.6/27.8/15.2/8.9 (BP = 0.946 ratio = 0.948 hyp_len = 105982 ref_len = 111811)\n",
            "2023-04-15 01:41:25 | INFO | hw5.seq2seq | saved epoch checkpoint: d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\checkpoints\\transformer-bt/checkpoint3.pt\n",
            "2023-04-15 01:41:25 | INFO | hw5.seq2seq | end of epoch 3\n",
            "2023-04-15 01:41:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1851\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c92bfe7e05349009dea76f43d269c73",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 4:   0%|          | 0/1851 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 01:48:01 | INFO | hw5.seq2seq | training loss: 3.6239\n",
            "2023-04-15 01:48:01 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dcdab90f4fb14316b70da9a2577c1fa6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 01:48:22 | INFO | hw5.seq2seq | example source: it's very destructive , but it also catalyzes the production of vitamin d in the skin , vitamin d being a molecule that we very much need for our strong bones , the health of our immune system , and myriad other important functions in our bodies .\n",
            "2023-04-15 01:48:22 | INFO | hw5.seq2seq | example hypothesis: 它非常破壞性 , 但它也催化了皮膚中的維生素d的產量 , 維生素d是一種分子 , 我們非常需要強烈的骨頭 , 我們免疫系統的健康 , 以及我們身體內其他重要的功能 。\n",
            "2023-04-15 01:48:22 | INFO | hw5.seq2seq | example reference: uvb非常的有破壞性 , 但是它也催化皮膚裡維生素d的生產 。 維生素d是我們非常需要的一種分子 , 它幫助強健骨骼、增強免疫系統、還有在我們身體裡行使其他各式各樣的重要功能 。\n",
            "2023-04-15 01:48:22 | INFO | hw5.seq2seq | validation loss:\t3.8674\n",
            "2023-04-15 01:48:22 | INFO | hw5.seq2seq | BLEU = 20.04 56.5/30.2/16.7/9.9 (BP = 0.869 ratio = 0.877 hyp_len = 98055 ref_len = 111811)\n",
            "2023-04-15 01:48:22 | INFO | hw5.seq2seq | saved epoch checkpoint: d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\checkpoints\\transformer-bt/checkpoint4.pt\n",
            "2023-04-15 01:48:22 | INFO | hw5.seq2seq | end of epoch 4\n",
            "2023-04-15 01:48:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1851\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f0fb93f69444782888d229255307532",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 5:   0%|          | 0/1851 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 01:54:58 | INFO | hw5.seq2seq | training loss: 3.5277\n",
            "2023-04-15 01:54:58 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e7eb53c5c7d41aa900b79bf5e08e9e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 01:55:19 | INFO | hw5.seq2seq | example source: it's what we call , \" invest , connect and celebrate . \"\n",
            "2023-04-15 01:55:19 | INFO | hw5.seq2seq | example hypothesis: 就是我們所謂的 「 投資、連結和慶祝 」\n",
            "2023-04-15 01:55:19 | INFO | hw5.seq2seq | example reference: 也就是我們稱之為『投資 , 連結 , 發揚光大』\n",
            "2023-04-15 01:55:19 | INFO | hw5.seq2seq | validation loss:\t3.7801\n",
            "2023-04-15 01:55:19 | INFO | hw5.seq2seq | BLEU = 21.35 55.5/29.5/16.4/9.7 (BP = 0.946 ratio = 0.947 hyp_len = 105892 ref_len = 111811)\n",
            "2023-04-15 01:55:19 | INFO | hw5.seq2seq | saved epoch checkpoint: d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\checkpoints\\transformer-bt/checkpoint5.pt\n",
            "2023-04-15 01:55:19 | INFO | hw5.seq2seq | end of epoch 5\n",
            "2023-04-15 01:55:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1851\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "437cdcc964be4ed295cf1c0197bf384e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 6:   0%|          | 0/1851 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 02:01:56 | INFO | hw5.seq2seq | training loss: 3.4700\n",
            "2023-04-15 02:01:56 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad4c36d4761f4b82a3a4162b21d58bfd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 02:02:19 | INFO | hw5.seq2seq | example source: thank you very much .\n",
            "2023-04-15 02:02:19 | INFO | hw5.seq2seq | example hypothesis: 謝謝大家 。\n",
            "2023-04-15 02:02:19 | INFO | hw5.seq2seq | example reference: 謝謝各位 。\n",
            "2023-04-15 02:02:19 | INFO | hw5.seq2seq | validation loss:\t3.7324\n",
            "2023-04-15 02:02:19 | INFO | hw5.seq2seq | BLEU = 22.07 55.1/29.5/16.6/9.8 (BP = 0.972 ratio = 0.973 hyp_len = 108777 ref_len = 111811)\n",
            "2023-04-15 02:02:19 | INFO | hw5.seq2seq | saved epoch checkpoint: d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\checkpoints\\transformer-bt/checkpoint6.pt\n",
            "2023-04-15 02:02:19 | INFO | hw5.seq2seq | end of epoch 6\n",
            "2023-04-15 02:02:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1851\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4e74a836cc44b94b24335af04865575",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 7:   0%|          | 0/1851 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 02:08:52 | INFO | hw5.seq2seq | training loss: 3.4277\n",
            "2023-04-15 02:08:52 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49459dca70ad4c948098776f347e8c5e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 02:09:13 | INFO | hw5.seq2seq | example source: thank you very much .\n",
            "2023-04-15 02:09:13 | INFO | hw5.seq2seq | example hypothesis: 非常感謝 。\n",
            "2023-04-15 02:09:13 | INFO | hw5.seq2seq | example reference: 非常感謝\n",
            "2023-04-15 02:09:13 | INFO | hw5.seq2seq | validation loss:\t3.6989\n",
            "2023-04-15 02:09:13 | INFO | hw5.seq2seq | BLEU = 21.99 56.9/30.8/17.4/10.4 (BP = 0.926 ratio = 0.929 hyp_len = 103846 ref_len = 111811)\n",
            "2023-04-15 02:09:14 | INFO | hw5.seq2seq | saved epoch checkpoint: d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\checkpoints\\transformer-bt/checkpoint7.pt\n",
            "2023-04-15 02:09:14 | INFO | hw5.seq2seq | end of epoch 7\n",
            "2023-04-15 02:09:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1851\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef07bbe564024a42b426d51fbb6b31d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 8:   0%|          | 0/1851 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 02:15:47 | INFO | hw5.seq2seq | training loss: 3.3955\n",
            "2023-04-15 02:15:47 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6504aab7397b42b6b9533b6459cf2499",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 02:16:09 | INFO | hw5.seq2seq | example source: some will go on for further education , but many will enter the workforce .\n",
            "2023-04-15 02:16:09 | INFO | hw5.seq2seq | example hypothesis: 有些人會更進一步的教育 , 但很多人會進入勞動力 。\n",
            "2023-04-15 02:16:09 | INFO | hw5.seq2seq | example reference: 有些人會繼續深造 , 但許多人會成為勞動力 。\n",
            "2023-04-15 02:16:09 | INFO | hw5.seq2seq | validation loss:\t3.6641\n",
            "2023-04-15 02:16:09 | INFO | hw5.seq2seq | BLEU = 22.38 56.6/30.7/17.4/10.4 (BP = 0.944 ratio = 0.945 hyp_len = 105696 ref_len = 111811)\n",
            "2023-04-15 02:16:09 | INFO | hw5.seq2seq | saved epoch checkpoint: d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\checkpoints\\transformer-bt/checkpoint8.pt\n",
            "2023-04-15 02:16:09 | INFO | hw5.seq2seq | end of epoch 8\n",
            "2023-04-15 02:16:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1851\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7236ac2500b74737a85ce826c20ee1c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 9:   0%|          | 0/1851 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 02:22:42 | INFO | hw5.seq2seq | training loss: 3.3715\n",
            "2023-04-15 02:22:42 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e38b6e3283014edf94d114eab34f9f01",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 02:23:05 | INFO | hw5.seq2seq | example source: what would i feel ?\n",
            "2023-04-15 02:23:05 | INFO | hw5.seq2seq | example hypothesis: 我會感受到什麼 ?\n",
            "2023-04-15 02:23:05 | INFO | hw5.seq2seq | example reference: 會有甚麼感覺呢 ?\n",
            "2023-04-15 02:23:05 | INFO | hw5.seq2seq | validation loss:\t3.6599\n",
            "2023-04-15 02:23:05 | INFO | hw5.seq2seq | BLEU = 22.80 56.3/30.7/17.4/10.4 (BP = 0.964 ratio = 0.964 hyp_len = 107815 ref_len = 111811)\n",
            "2023-04-15 02:23:05 | INFO | hw5.seq2seq | saved epoch checkpoint: d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\checkpoints\\transformer-bt/checkpoint9.pt\n",
            "2023-04-15 02:23:05 | INFO | hw5.seq2seq | end of epoch 9\n",
            "2023-04-15 02:23:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1851\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "243ec4a69e8a46e78dae7fda8d876dd8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 10:   0%|          | 0/1851 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 02:29:34 | INFO | hw5.seq2seq | training loss: 3.3527\n",
            "2023-04-15 02:29:34 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7af16089b0054bd89ce0ffbf795814a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 02:29:57 | INFO | hw5.seq2seq | example source: so think about when you have friends , families and coworkers in california , on the west coast or in other parts of the world .\n",
            "2023-04-15 02:29:57 | INFO | hw5.seq2seq | example hypothesis: 所以 , 想想看 , 當你有朋友、家庭和同事在加州 , 在西岸或其他地區 ,\n",
            "2023-04-15 02:29:57 | INFO | hw5.seq2seq | example reference: 所以 , 想像當你嘗試聯繫在加州 , 在西海岸或者在世界的另一面\n",
            "2023-04-15 02:29:57 | INFO | hw5.seq2seq | validation loss:\t3.6570\n",
            "2023-04-15 02:29:57 | INFO | hw5.seq2seq | BLEU = 22.96 55.0/29.8/16.9/10.1 (BP = 1.000 ratio = 1.006 hyp_len = 112494 ref_len = 111811)\n",
            "2023-04-15 02:29:58 | INFO | hw5.seq2seq | saved epoch checkpoint: d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\checkpoints\\transformer-bt/checkpoint10.pt\n",
            "2023-04-15 02:29:58 | INFO | hw5.seq2seq | end of epoch 10\n",
            "2023-04-15 02:29:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1851\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "615ed57ba137471fbee21df0a9239c08",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 11:   0%|          | 0/1851 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 02:36:23 | INFO | hw5.seq2seq | training loss: 3.3368\n",
            "2023-04-15 02:36:23 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "abb3c79902df4d32b0872ac3eb56e711",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 02:36:44 | INFO | hw5.seq2seq | example source: okay , so by this logic , bad is always stronger than good when it comes to updating .\n",
            "2023-04-15 02:36:44 | INFO | hw5.seq2seq | example hypothesis: 好 , 所以 , 藉由這個邏輯 , 壞事總是比起在上升時更強烈 。\n",
            "2023-04-15 02:36:44 | INFO | hw5.seq2seq | example reference: 好 , 依據這樣的邏輯當有新的印象出現時壞的效果比好的效果來得強烈\n",
            "2023-04-15 02:36:44 | INFO | hw5.seq2seq | validation loss:\t3.6328\n",
            "2023-04-15 02:36:44 | INFO | hw5.seq2seq | BLEU = 23.02 56.4/30.8/17.6/10.7 (BP = 0.963 ratio = 0.964 hyp_len = 107734 ref_len = 111811)\n",
            "2023-04-15 02:36:44 | INFO | hw5.seq2seq | saved epoch checkpoint: d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\checkpoints\\transformer-bt/checkpoint11.pt\n",
            "2023-04-15 02:36:44 | INFO | hw5.seq2seq | end of epoch 11\n",
            "2023-04-15 02:36:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1851\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12a618e39d5245f2978eda56d6f61f20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 12:   0%|          | 0/1851 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 02:43:13 | INFO | hw5.seq2seq | training loss: 3.3227\n",
            "2023-04-15 02:43:13 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74ad3a81f6bb4d6387bb86585d18d025",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 02:43:36 | INFO | hw5.seq2seq | example source: never mind the fact that the address led to a strip mall somewhere in northern l . a .\n",
            "2023-04-15 02:43:36 | INFO | hw5.seq2seq | example hypothesis: 從來沒有想過地址導致北l.a某處的嚴格商場 。\n",
            "2023-04-15 02:43:36 | INFO | hw5.seq2seq | example reference: 但事實是這個公司位於洛杉磯北部的某購物中心\n",
            "2023-04-15 02:43:36 | INFO | hw5.seq2seq | validation loss:\t3.6267\n",
            "2023-04-15 02:43:36 | INFO | hw5.seq2seq | BLEU = 23.11 56.7/31.0/17.7/10.6 (BP = 0.964 ratio = 0.965 hyp_len = 107902 ref_len = 111811)\n",
            "2023-04-15 02:43:36 | INFO | hw5.seq2seq | saved epoch checkpoint: d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\checkpoints\\transformer-bt/checkpoint12.pt\n",
            "2023-04-15 02:43:36 | INFO | hw5.seq2seq | end of epoch 12\n",
            "2023-04-15 02:43:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1851\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "64b179b9159047609ccf26a3795ece26",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 13:   0%|          | 0/1851 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 02:50:04 | INFO | hw5.seq2seq | training loss: 3.3094\n",
            "2023-04-15 02:50:04 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "38419ae4ce3c459db0a7e690f83b21a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 02:50:26 | INFO | hw5.seq2seq | example source: we wear all black , we get very depressed , you think we're adorable , we're dead inside because we've got no choice .\n",
            "2023-04-15 02:50:26 | INFO | hw5.seq2seq | example hypothesis: 我們穿著所有的黑色 , 我們變得非常沮喪 , 你覺得我們是可愛的 , 我們死在裡面 , 因為我們沒有選擇 。\n",
            "2023-04-15 02:50:26 | INFO | hw5.seq2seq | example reference: 我們身著華服你們認為我們是值得尊敬的我們內心是一片死寂 , 因為我們別無選擇\n",
            "2023-04-15 02:50:26 | INFO | hw5.seq2seq | validation loss:\t3.6096\n",
            "2023-04-15 02:50:26 | INFO | hw5.seq2seq | BLEU = 23.25 55.9/30.5/17.3/10.4 (BP = 0.987 ratio = 0.987 hyp_len = 110410 ref_len = 111811)\n",
            "2023-04-15 02:50:26 | INFO | hw5.seq2seq | saved epoch checkpoint: d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\checkpoints\\transformer-bt/checkpoint13.pt\n",
            "2023-04-15 02:50:26 | INFO | hw5.seq2seq | end of epoch 13\n",
            "2023-04-15 02:50:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1851\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a80e53aa397a4835b5f554dbb716f3e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 14:   0%|          | 0/1851 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 02:56:52 | INFO | hw5.seq2seq | training loss: 3.3005\n",
            "2023-04-15 02:56:52 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c46682319cc34b9884700d4244d33488",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 02:57:12 | INFO | hw5.seq2seq | example source: he's saying to me , 'electricity . ' was he an electrician ? \" \" no . \"\n",
            "2023-04-15 02:57:12 | INFO | hw5.seq2seq | example hypothesis: 他對我說: 「 電力 。 他是電器嗎 ? 」\n",
            "2023-04-15 02:57:12 | INFO | hw5.seq2seq | example reference: 他對我說『電流』他是一個電工嗎 ? 」 不是\n",
            "2023-04-15 02:57:12 | INFO | hw5.seq2seq | validation loss:\t3.5916\n",
            "2023-04-15 02:57:12 | INFO | hw5.seq2seq | BLEU = 22.87 57.5/31.4/17.9/10.9 (BP = 0.939 ratio = 0.941 hyp_len = 105216 ref_len = 111811)\n",
            "2023-04-15 02:57:12 | INFO | hw5.seq2seq | saved epoch checkpoint: d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\checkpoints\\transformer-bt/checkpoint14.pt\n",
            "2023-04-15 02:57:12 | INFO | hw5.seq2seq | end of epoch 14\n",
            "2023-04-15 02:57:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1851\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ca21e45c9134a2694c8f48dc20535c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 15:   0%|          | 0/1851 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 03:03:38 | INFO | hw5.seq2seq | training loss: 3.2912\n",
            "2023-04-15 03:03:38 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1febd1b5134c4f7e9bbe6528c279c3bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 03:04:00 | INFO | hw5.seq2seq | example source: so they went to their data science team , and they were able to scale this big data insight in with their quantitative data .\n",
            "2023-04-15 03:04:00 | INFO | hw5.seq2seq | example hypothesis: 所以他們去了他們的數據科學團隊 , 他們能夠用量化資料來衡量這個大數據 。\n",
            "2023-04-15 03:04:00 | INFO | hw5.seq2seq | example reference: 於是叫他們的數據科學組把這洞察放大到量化數據的規模來衡量 。\n",
            "2023-04-15 03:04:00 | INFO | hw5.seq2seq | validation loss:\t3.5937\n",
            "2023-04-15 03:04:00 | INFO | hw5.seq2seq | BLEU = 23.34 56.6/31.1/17.8/10.8 (BP = 0.969 ratio = 0.969 hyp_len = 108345 ref_len = 111811)\n",
            "2023-04-15 03:04:00 | INFO | hw5.seq2seq | saved epoch checkpoint: d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\checkpoints\\transformer-bt/checkpoint15.pt\n",
            "2023-04-15 03:04:00 | INFO | hw5.seq2seq | end of epoch 15\n",
            "2023-04-15 03:04:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1851\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "854e0ac03dea46d59d5c9cdc47c2ca15",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 16:   0%|          | 0/1851 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 03:10:25 | INFO | hw5.seq2seq | training loss: 3.2811\n",
            "2023-04-15 03:10:25 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc5b0094999049f1b498e8476cd99064",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 03:10:45 | INFO | hw5.seq2seq | example source: in the 35 years i've been a climbing guide and taught on indoor walls , and stuff like that , the most important thing i've learned was , guys will always try to do pullups .\n",
            "2023-04-15 03:10:45 | INFO | hw5.seq2seq | example hypothesis: 在三十五年來我一直在攀爬指導並在室內牆上教導 , 諸如此類的事情 , 我學到的最重要的是 , 各位總是會試著去做肺部 。\n",
            "2023-04-15 03:10:45 | INFO | hw5.seq2seq | example reference: 35年來我一直擔任攀岩嚮導在室內岩場教學 , 並做些像這樣的工作 , 而我所學到的最重要的事情就是 , 男性們通常會做引體向上 。\n",
            "2023-04-15 03:10:45 | INFO | hw5.seq2seq | validation loss:\t3.5717\n",
            "2023-04-15 03:10:45 | INFO | hw5.seq2seq | BLEU = 23.27 57.4/31.7/18.1/11.0 (BP = 0.948 ratio = 0.950 hyp_len = 106176 ref_len = 111811)\n",
            "2023-04-15 03:10:45 | INFO | hw5.seq2seq | saved epoch checkpoint: d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\checkpoints\\transformer-bt/checkpoint16.pt\n",
            "2023-04-15 03:10:45 | INFO | hw5.seq2seq | end of epoch 16\n",
            "2023-04-15 03:10:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1851\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01e873e991624b0193c85a18039d4b41",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 17:   0%|          | 0/1851 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 03:17:05 | INFO | hw5.seq2seq | training loss: 3.2722\n",
            "2023-04-15 03:17:05 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56eddf682b98419f8efcb09e9a6ee0f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 03:17:28 | INFO | hw5.seq2seq | example source: who gets safe surgery and who doesn't ?\n",
            "2023-04-15 03:17:28 | INFO | hw5.seq2seq | example hypothesis: 誰得到安全的手術 , 以及誰沒有 ?\n",
            "2023-04-15 03:17:28 | INFO | hw5.seq2seq | example reference: 誰能接受安全的手術誰不能 ?\n",
            "2023-04-15 03:17:28 | INFO | hw5.seq2seq | validation loss:\t3.5737\n",
            "2023-04-15 03:17:28 | INFO | hw5.seq2seq | BLEU = 23.40 57.0/31.4/18.0/10.9 (BP = 0.963 ratio = 0.963 hyp_len = 107729 ref_len = 111811)\n",
            "2023-04-15 03:17:28 | INFO | hw5.seq2seq | saved epoch checkpoint: d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\checkpoints\\transformer-bt/checkpoint17.pt\n",
            "2023-04-15 03:17:28 | INFO | hw5.seq2seq | end of epoch 17\n",
            "2023-04-15 03:17:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1851\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e6db389ad7442e38320499234806c4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 18:   0%|          | 0/1851 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 03:23:45 | INFO | hw5.seq2seq | training loss: 3.2681\n",
            "2023-04-15 03:23:45 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44e6d57f8df8455997c70be5905484d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 03:24:07 | INFO | hw5.seq2seq | example source: they breathe easier .\n",
            "2023-04-15 03:24:07 | INFO | hw5.seq2seq | example hypothesis: 他們呼吸更容易\n",
            "2023-04-15 03:24:07 | INFO | hw5.seq2seq | example reference: 他們比較能呼吸 。\n",
            "2023-04-15 03:24:07 | INFO | hw5.seq2seq | validation loss:\t3.5535\n",
            "2023-04-15 03:24:07 | INFO | hw5.seq2seq | BLEU = 23.37 56.9/31.2/17.9/10.8 (BP = 0.965 ratio = 0.966 hyp_len = 107981 ref_len = 111811)\n",
            "2023-04-15 03:24:07 | INFO | hw5.seq2seq | saved epoch checkpoint: d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\checkpoints\\transformer-bt/checkpoint18.pt\n",
            "2023-04-15 03:24:07 | INFO | hw5.seq2seq | end of epoch 18\n",
            "2023-04-15 03:24:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1851\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf11e205d89d4434a51bf2198f7889ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 19:   0%|          | 0/1851 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 03:30:34 | INFO | hw5.seq2seq | training loss: 3.2613\n",
            "2023-04-15 03:30:34 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eff2ae127af84dd0a37841c5ca05ebf9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 03:30:56 | INFO | hw5.seq2seq | example source: all of the world's defense budgets and military spending taken together total just under two trillion dollars per year .\n",
            "2023-04-15 03:30:56 | INFO | hw5.seq2seq | example hypothesis: 世界上所有的防禦預算和軍隊都花在每年兩兆美元下 。\n",
            "2023-04-15 03:30:56 | INFO | hw5.seq2seq | example reference: 全球的軍事預算加總起來 , 每年都不到兩兆美金 。\n",
            "2023-04-15 03:30:56 | INFO | hw5.seq2seq | validation loss:\t3.5631\n",
            "2023-04-15 03:30:56 | INFO | hw5.seq2seq | BLEU = 23.70 56.6/31.2/17.9/10.9 (BP = 0.979 ratio = 0.979 hyp_len = 109482 ref_len = 111811)\n",
            "2023-04-15 03:30:56 | INFO | hw5.seq2seq | saved epoch checkpoint: d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\checkpoints\\transformer-bt/checkpoint19.pt\n",
            "2023-04-15 03:30:56 | INFO | hw5.seq2seq | end of epoch 19\n",
            "2023-04-15 03:30:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1851\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ac08fcc1ba645a5bfb9498fa56508ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 20:   0%|          | 0/1851 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 03:37:20 | INFO | hw5.seq2seq | training loss: 3.2556\n",
            "2023-04-15 03:37:20 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04ae04b66d9c43fba7d9ce4946e8e793",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 03:37:41 | INFO | hw5.seq2seq | example source: how do these nielsen ratings reflect not just what you've heard about , which is the idea of our social , collective unconscious , but how do these top10 nielsenrated shows over 50 years reflect the idea of our social conscience ?\n",
            "2023-04-15 03:37:41 | INFO | hw5.seq2seq | example hypothesis: 這些尼爾森評級如何反映出的不只是你聽過的 , 哪些是我們社會、集體不自覺的想法 , 而是這些頂尖的10個nielsensensenenenenenenens的節目是如何反映出我們社會科學的想法 ?\n",
            "2023-04-15 03:37:41 | INFO | hw5.seq2seq | example reference: 尼爾森收視率調查如何反應出所謂的社會集體無意識 ? 尼爾森收視率調查又是如何在過去五十年中反應出我們社會的意識 ?\n",
            "2023-04-15 03:37:41 | INFO | hw5.seq2seq | validation loss:\t3.5592\n",
            "2023-04-15 03:37:41 | INFO | hw5.seq2seq | BLEU = 23.82 56.8/31.4/18.1/11.0 (BP = 0.977 ratio = 0.977 hyp_len = 109250 ref_len = 111811)\n",
            "2023-04-15 03:37:41 | INFO | hw5.seq2seq | saved epoch checkpoint: d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\checkpoints\\transformer-bt/checkpoint20.pt\n",
            "2023-04-15 03:37:41 | INFO | hw5.seq2seq | end of epoch 20\n",
            "2023-04-15 03:37:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1851\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce0c88b4be4547daa289d26d31cd30dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 21:   0%|          | 0/1851 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 03:43:58 | INFO | hw5.seq2seq | training loss: 3.2504\n",
            "2023-04-15 03:43:58 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ae6e396cff4148b7b903d9bf46e31d93",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 03:44:20 | INFO | hw5.seq2seq | example source: moreover , we can calculate a probability that the universe was created in different states .\n",
            "2023-04-15 03:44:20 | INFO | hw5.seq2seq | example hypothesis: 此外 , 我們可以計算出宇宙在不同狀態所創造的機率 。\n",
            "2023-04-15 03:44:20 | INFO | hw5.seq2seq | example reference: 不同狀態的或然率 。 這些預測可以由\n",
            "2023-04-15 03:44:20 | INFO | hw5.seq2seq | validation loss:\t3.5548\n",
            "2023-04-15 03:44:20 | INFO | hw5.seq2seq | BLEU = 23.91 56.2/30.9/17.8/10.9 (BP = 0.993 ratio = 0.993 hyp_len = 111031 ref_len = 111811)\n",
            "2023-04-15 03:44:20 | INFO | hw5.seq2seq | saved epoch checkpoint: d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\checkpoints\\transformer-bt/checkpoint21.pt\n",
            "2023-04-15 03:44:20 | INFO | hw5.seq2seq | end of epoch 21\n",
            "2023-04-15 03:44:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1851\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a490e7734bf4a36a2d61ee70c7171af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 22:   0%|          | 0/1851 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 03:50:32 | INFO | hw5.seq2seq | training loss: 3.2452\n",
            "2023-04-15 03:50:32 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d8cd77230064df08dab77aa38879103",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 03:50:53 | INFO | hw5.seq2seq | example source: everything i do is stitched with its color . \"\n",
            "2023-04-15 03:50:53 | INFO | hw5.seq2seq | example hypothesis: 我所做的每件事都是用它的顏色儲存下來 。 」\n",
            "2023-04-15 03:50:53 | INFO | hw5.seq2seq | example reference: 我的行為都縫上了你的色彩 。 」\n",
            "2023-04-15 03:50:53 | INFO | hw5.seq2seq | validation loss:\t3.5291\n",
            "2023-04-15 03:50:53 | INFO | hw5.seq2seq | BLEU = 23.77 56.7/31.2/17.9/10.9 (BP = 0.981 ratio = 0.981 hyp_len = 109669 ref_len = 111811)\n",
            "2023-04-15 03:50:53 | INFO | hw5.seq2seq | saved epoch checkpoint: d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\checkpoints\\transformer-bt/checkpoint22.pt\n",
            "2023-04-15 03:50:53 | INFO | hw5.seq2seq | end of epoch 22\n",
            "2023-04-15 03:50:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1851\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d9f465395404d898aa913fa08909c91",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 23:   0%|          | 0/1851 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 03:57:12 | INFO | hw5.seq2seq | training loss: 3.2425\n",
            "2023-04-15 03:57:12 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d268c34f5cd41e0be416e9d4c2aae90",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 03:57:32 | INFO | hw5.seq2seq | example source: the lesson that we learned from the soviet debacle is that only by a miracle will the working poor be reempowered , as they were in ancient athens , without creating new forms of brutality and waste .\n",
            "2023-04-15 03:57:32 | INFO | hw5.seq2seq | example hypothesis: 我們從蘇聯戰役中學到的教訓是 , 只有一個奇蹟會讓工作窮人被重新塑造 , 當他們在古老的雅典時 , 沒有創造出新的殘酷和浪費形式 。\n",
            "2023-04-15 03:57:32 | INFO | hw5.seq2seq | example reference: 我們從蘇聯解體吸取到的教訓是 , 只有奇蹟發生 , 貧窮的勞工才能被重新賦權 , 就像古老雅典時代那樣 , 而不會創造新形式的暴行和浪費 。\n",
            "2023-04-15 03:57:32 | INFO | hw5.seq2seq | validation loss:\t3.5592\n",
            "2023-04-15 03:57:32 | INFO | hw5.seq2seq | BLEU = 23.64 57.0/31.4/18.0/10.9 (BP = 0.970 ratio = 0.971 hyp_len = 108514 ref_len = 111811)\n",
            "2023-04-15 03:57:32 | INFO | hw5.seq2seq | saved epoch checkpoint: d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\checkpoints\\transformer-bt/checkpoint23.pt\n",
            "2023-04-15 03:57:32 | INFO | hw5.seq2seq | end of epoch 23\n",
            "2023-04-15 03:57:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1851\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27fd0fdfe98b4683b68398f9013e8c32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 24:   0%|          | 0/1851 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 04:03:34 | INFO | hw5.seq2seq | training loss: 3.2365\n",
            "2023-04-15 04:03:34 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d75bed367a704cb48322700b305287a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 04:03:54 | INFO | hw5.seq2seq | example source: a few years ago , my parents and i went to see the rockettes , radio city's highkicking dancers .\n",
            "2023-04-15 04:03:54 | INFO | hw5.seq2seq | example hypothesis: 幾年前 , 我父母和我去看火箭 , 無線電城的高踢舞者 。\n",
            "2023-04-15 04:03:54 | INFO | hw5.seq2seq | example reference: 幾年前 , 我父母和我去看rockettes舞團 , 無線電城音樂廳中腿能踢很高的舞者們 。\n",
            "2023-04-15 04:03:54 | INFO | hw5.seq2seq | validation loss:\t3.5374\n",
            "2023-04-15 04:03:54 | INFO | hw5.seq2seq | BLEU = 23.73 56.9/31.4/18.0/11.0 (BP = 0.974 ratio = 0.974 hyp_len = 108887 ref_len = 111811)\n",
            "2023-04-15 04:03:54 | INFO | hw5.seq2seq | saved epoch checkpoint: d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\checkpoints\\transformer-bt/checkpoint24.pt\n",
            "2023-04-15 04:03:54 | INFO | hw5.seq2seq | end of epoch 24\n",
            "2023-04-15 04:03:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1851\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49d048d288fb43be83839352c0c23098",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 25:   0%|          | 0/1851 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 04:10:06 | INFO | hw5.seq2seq | training loss: 3.2345\n",
            "2023-04-15 04:10:06 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec9e2e7cc35e466f9c3fb5b456f12958",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 04:10:26 | INFO | hw5.seq2seq | example source: and reality and i we don't share the same values , the same goals to be honest , i don't have goals ; i have fantasies .\n",
            "2023-04-15 04:10:26 | INFO | hw5.seq2seq | example hypothesis: 現實和我沒有共享相同的價值觀 , 相同的目標老實說 , 我沒有目標 ; 我有幻想 。\n",
            "2023-04-15 04:10:26 | INFO | hw5.seq2seq | example reference: 而現實和我我們沒有相同的價值觀、相同的目標老實說 , 我沒有目標 ; 我有幻想 。\n",
            "2023-04-15 04:10:26 | INFO | hw5.seq2seq | validation loss:\t3.5302\n",
            "2023-04-15 04:10:26 | INFO | hw5.seq2seq | BLEU = 23.85 57.1/31.7/18.3/11.2 (BP = 0.966 ratio = 0.967 hyp_len = 108100 ref_len = 111811)\n",
            "2023-04-15 04:10:27 | INFO | hw5.seq2seq | saved epoch checkpoint: d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\checkpoints\\transformer-bt/checkpoint25.pt\n",
            "2023-04-15 04:10:27 | INFO | hw5.seq2seq | end of epoch 25\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No improvements in the recent 3 epochs. So far the best BLEU score=23.91494\n"
          ]
        }
      ],
      "source": [
        "epoch_itr = load_data_iterator(task, \"train\", config.start_epoch, config.max_tokens, config.num_workers)\n",
        "try_load_checkpoint(model, optimizer, name=config.resume)\n",
        "bleu_best, early_stop_cnt = 0.0, 0\n",
        "gnorms = []\n",
        "while epoch_itr.next_epoch_idx <= config.max_epoch:\n",
        "    # train for one epoch\n",
        "    train_stats = train_one_epoch(epoch_itr, model, task, criterion, optimizer, config.accum_steps)\n",
        "    stats = validate_and_save(model, task, criterion, optimizer, epoch=epoch_itr.epoch)\n",
        "    logger.info(\"end of epoch {}\".format(epoch_itr.epoch))    \n",
        "    epoch_itr = load_data_iterator(task, \"train\", epoch_itr.next_epoch_idx, config.max_tokens, config.num_workers)\n",
        "    \n",
        "    gnorms += train_stats[\"gnorm\"]\n",
        "    \n",
        "    if stats[\"bleu\"].score > bleu_best:\n",
        "        bleu_best = stats[\"bleu\"].score\n",
        "        early_stop_cnt = 0\n",
        "    else:\n",
        "        early_stop_cnt += 1\n",
        "    \n",
        "    if early_stop_cnt > config.early_stop_patience:\n",
        "        print(f\"No improvements in the recent {config.early_stop_patience} epochs. So far the best BLEU score={bleu_best:.5f}\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(20, 10))\n",
        "plt.plot(gnorms)\n",
        "plt.plot([config.clip_norm] * len(gnorms), \"--\")\n",
        "plt.xlim(0)\n",
        "plt.ylim(0)\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Grad norm\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 04:10:27 | INFO | hw5.seq2seq | loaded checkpoint checkpoints\\transformer-bt\\checkpoint_best.pt: step=unknown loss=3.5547916889190674 bleu=23.91493534342627\n"
          ]
        }
      ],
      "source": [
        "try_load_checkpoint(model, name=\"checkpoint_best.pt\")\n",
        "\n",
        "pos_emb = model.decoder.embed_positions.weights.cpu().detach()\n",
        "pos_emb_n = F.normalize(pos_emb, p=2, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(pos_emb_n @ pos_emb_n.T)\n",
        "plt.colorbar(fraction=0.046, pad=0.04)\n",
        "plt.title(\"Position Embedding Cosine Similarity\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyjRwllxPjtf"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "N70Gc6smPi1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(inputs=['./checkpoints/transformer-bt'], output='./checkpoints/transformer-bt/avg_last_5_checkpoint.pt', num_epoch_checkpoints=5, num_update_checkpoints=None, checkpoint_upper_bound=None)\n",
            "averaging checkpoints:  ['./checkpoints/transformer-bt\\\\checkpoint25.pt', './checkpoints/transformer-bt\\\\checkpoint24.pt', './checkpoints/transformer-bt\\\\checkpoint23.pt', './checkpoints/transformer-bt\\\\checkpoint22.pt', './checkpoints/transformer-bt\\\\checkpoint21.pt']\n",
            "Finished writing averaged checkpoint to ./checkpoints/transformer-bt/avg_last_5_checkpoint.pt\n"
          ]
        }
      ],
      "source": [
        "# averaging a few checkpoints can have a similar effect to ensemble\n",
        "checkdir=config.savedir\n",
        "!python ./fairseq/scripts/average_checkpoints.py \\\n",
        "--inputs {checkdir} \\\n",
        "--num-epoch-checkpoints 5 \\\n",
        "--output {checkdir}/avg_last_5_checkpoint.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAGMiun8PnZy"
      },
      "source": [
        "## Confirm model weights used to generate submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "tvRdivVUPnsU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 04:10:31 | INFO | hw5.seq2seq | loaded checkpoint checkpoints\\transformer-bt\\avg_last_5_checkpoint.pt: step=unknown loss=3.530181407928467 bleu=23.847164376260757\n",
            "2023-04-15 04:10:31 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6083997a00e447f0b55ec1a04fce3256",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 04:10:51 | INFO | hw5.seq2seq | example source: we can't .\n",
            "2023-04-15 04:10:51 | INFO | hw5.seq2seq | example hypothesis: 我們不能\n",
            "2023-04-15 04:10:51 | INFO | hw5.seq2seq | example reference: 這是行不通的 。\n",
            "2023-04-15 04:10:51 | INFO | hw5.seq2seq | validation loss:\t3.5276\n",
            "2023-04-15 04:10:51 | INFO | hw5.seq2seq | BLEU = 23.88 56.9/31.4/18.1/11.0 (BP = 0.978 ratio = 0.978 hyp_len = 109387 ref_len = 111811)\n"
          ]
        }
      ],
      "source": [
        "# checkpoint_last.pt : latest epoch\n",
        "# checkpoint_best.pt : highest validation bleu\n",
        "# avg_last_5_checkpoint.pt:　the average of last 5 epochs\n",
        "try_load_checkpoint(model, name=\"avg_last_5_checkpoint.pt\")\n",
        "validate(model, task, criterion, log_to_wandb=False)\n",
        "None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioAIflXpPsxt"
      },
      "source": [
        "## Generate Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "oYMxA8FlPtIq"
      },
      "outputs": [],
      "source": [
        "def generate_prediction(model, task, split=\"test\", outfile=\"./prediction.txt\"):    \n",
        "    task.load_dataset(split=split, epoch=1)\n",
        "    itr = load_data_iterator(task, split, 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
        "    \n",
        "    idxs = []\n",
        "    hyps = []\n",
        "\n",
        "    model.eval()\n",
        "    progress = tqdm.tqdm(itr, desc=f\"prediction\")\n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(progress):\n",
        "            # validation loss\n",
        "            sample = utils.move_to_cuda(sample, device=device)\n",
        "\n",
        "            # do inference\n",
        "            s, h, r = inference_step(sample, model)\n",
        "            \n",
        "            hyps.extend(h)\n",
        "            idxs.extend(list(sample['id']))\n",
        "            \n",
        "    # sort based on the order before preprocess\n",
        "    hyps = [x for _,x in sorted(zip(idxs,hyps))]\n",
        "    \n",
        "    with open(outfile, \"w\") as f:\n",
        "        for h in hyps:\n",
        "            f.write(h+\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Le4RFWXxjmm0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 04:10:51 | INFO | fairseq.data.data_utils | loaded 4,000 examples from: ./DATA/data-bin/ted2020_with_mono\\test.en-zh.en\n",
            "2023-04-15 04:10:52 | INFO | fairseq.data.data_utils | loaded 4,000 examples from: ./DATA/data-bin/ted2020_with_mono\\test.en-zh.zh\n",
            "2023-04-15 04:10:52 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020_with_mono test en-zh 4000 examples\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4dc9345f8ab745cf9f2cf3fdc50f456f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "prediction:   0%|          | 0/17 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "generate_prediction(model, task)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z0cJE-wPzaU"
      },
      "source": [
        "# Back-translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-7uPJ2CP0sm"
      },
      "source": [
        "## Train a backward translation model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppGHjg2ZP3sV"
      },
      "source": [
        "1. Switch the source_lang and target_lang in **config** \n",
        "2. Change the savedir in **config** (eg. \"./checkpoints/transformer-back\")\n",
        "3. Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waTGz29UP6WI"
      },
      "source": [
        "## Generate synthetic data with backward model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIeTsPexP8FL"
      },
      "source": [
        "### Download monolingual data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "i7N4QlsbP8fh"
      },
      "outputs": [],
      "source": [
        "mono_dataset_name = 'mono'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "396saD9-QBPY"
      },
      "outputs": [],
      "source": [
        "mono_prefix = Path(data_dir).absolute() / mono_dataset_name\n",
        "mono_prefix.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# urls = (\n",
        "#     \"https://github.com/yuhsinchan/ML2022-HW5Dataset/releases/download/v1.0.2/ted_zh_corpus.deduped.gz\",\n",
        "# )\n",
        "# file_names = (\n",
        "#     'ted_zh_corpus.deduped.gz',\n",
        "# )\n",
        "\n",
        "# for u, f in zip(urls, file_names):\n",
        "#     path = mono_prefix/f\n",
        "#     if not path.exists():\n",
        "#         !wget \"{u}\" -O \"{path}\"\n",
        "#     else:\n",
        "#         print(f'{f} is exist, skip downloading')\n",
        "#     if path.suffix == \".tgz\":\n",
        "#         !tar -xvf \"{path}\" -C \"{prefix}\"\n",
        "#     elif path.suffix == \".zip\":\n",
        "#         !unzip -o \"{path}\" -d \"{prefix}\"\n",
        "#     elif path.suffix == \".gz\":\n",
        "#         !gzip -fkd \"{path}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOVQRHzGQU4-"
      },
      "source": [
        "### TODO: clean corpus\n",
        "\n",
        "1. remove sentences that are too long or too short\n",
        "2. unify punctuation\n",
        "\n",
        "hint: you can use clean_s() defined above to do this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "eIYmxfUOQSov"
      },
      "outputs": [],
      "source": [
        "data_mono_prefix = os.path.join(mono_prefix, \"ted_zh_corpus.deduped\")\n",
        "\n",
        "max_len=1000\n",
        "min_len=1\n",
        "\n",
        "if Path(f'{data_mono_prefix}.clean.zh').exists() and Path(f'{data_mono_prefix}.clean.en').exists():\n",
        "    print(f'{data_mono_prefix}.clean.zh & en exists. skipping clean.')\n",
        "else: \n",
        "    with open(f'{data_mono_prefix}', 'r') as in_f:\n",
        "        with open(f'{data_mono_prefix}.clean.zh', 'w') as out_f_zh:\n",
        "            with open(f'{data_mono_prefix}.clean.en', 'w') as out_f_en: # Need to create a dummy file due to the stupid design of 'generate_prediction'\n",
        "                for s in in_f:\n",
        "                    s = s.strip()\n",
        "                    s = clean_s(s, \"zh\")\n",
        "                    s_len = len_s(s, \"zh\")\n",
        "                    if min_len > 0: # remove short sentence\n",
        "                        if s_len < min_len:\n",
        "                            continue\n",
        "                    if max_len > 0: # remove long sentence\n",
        "                        if s_len > max_len:\n",
        "                            continue\n",
        "                    print(s, file=out_f_zh)\n",
        "                    print(\".\", file=out_f_en)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "在16世紀中葉意大利人被一種男歌手迷住了那種男歌手的音域廣闊 , 包含的音高先前是一般成年男性不可能達到的\n",
            "但是 , 這天賦有一個很高的代價\n",
            "要防止他們變聲這些歌手在青春期前被閹割來停止荷爾蒙的變化 , 以免他們的聲線變低沉\n",
            "被稱為 「 閹伶 」 , 他們輕輕的、天使般的聲音在整個歐洲很有名直到這個殘酷的程序 , 在19世紀被禁止\n",
            "雖然阻止聲帶的成長 , 可以產生一個非凡廣闊的音域但自然發展的聲音 , 已經具有極多的可能性\n"
          ]
        }
      ],
      "source": [
        "!head \"{data_mono_prefix+'.clean.zh'}\" -n 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jegH0bvMQVmR"
      },
      "source": [
        "### TODO: Subword Units\n",
        "\n",
        "Use the spm model of the backward model to tokenize the data into subword units\n",
        "\n",
        "hint: spm model is located at DATA/raw-data/\\[dataset\\]/spm\\[vocab_num\\].model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "vqgR4uUMQZGY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\DATA\\rawdata\\mono\\mono.tok.zh exists. skipping spm_encode.\n",
            "d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\DATA\\rawdata\\mono\\mono.tok.en exists. skipping spm_encode.\n"
          ]
        }
      ],
      "source": [
        "spm_model = spm.SentencePieceProcessor(model_file=str(prefix/f'spm{vocab_size}.model'))\n",
        "\n",
        "for lang in [ \"zh\", \"en\" ]:\n",
        "    mono_out_path = mono_prefix / f\"mono.tok.{lang}\"\n",
        "    if mono_out_path.exists():\n",
        "        print(f\"{mono_out_path} exists. skipping spm_encode.\")\n",
        "    else:\n",
        "        with open(mono_out_path, 'w') as out_f:\n",
        "            with open(f'{data_mono_prefix}.clean.{lang}', 'r') as in_f:\n",
        "                for line in in_f:\n",
        "                    line = line.strip()\n",
        "                    tok = spm_model.encode(line, out_type=str)\n",
        "                    print(' '.join(tok), file=out_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▁在 16 世紀 中 葉 意 大 利 人 被 一種 男 歌 手 迷 住 了 那種 男 歌 手 的 音 域 廣 闊 ▁, ▁ 包 含 的 音 高 先 前 是 一般 成 年 男性 不可能 達到 的\n",
            "▁但是 ▁, ▁這 天 賦 有一個 很 高 的 代 價\n",
            "▁要 防 止 他們 變 聲 這些 歌 手 在 青 春 期 前 被 閹 割 來 停 止 荷 爾 蒙 的 變化 ▁, ▁以 免 他們的 聲 線 變 低 沉\n",
            "▁ 被 稱為 ▁「 ▁ 閹 伶 ▁」 ▁, ▁他們 輕 輕 的 、 天 使 般 的 聲音 在 整個 歐 洲 很 有 名 直 到 這個 殘 酷 的 程 序 ▁, ▁在 19 世紀 被 禁 止\n",
            "▁雖然 阻 止 聲 帶 的 成長 ▁, ▁ 可以 產生 一個 非 凡 廣 闊 的 音 域 但 自然 發展 的 聲音 ▁, ▁ 已經 具有 極 多 的 可能 性\n"
          ]
        }
      ],
      "source": [
        "!head \"{mono_prefix / 'mono.tok.zh'}\" -n 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a65glBVXQZiE"
      },
      "source": [
        "### Binarize\n",
        "\n",
        "use fairseq to binarize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "b803qA5aQaEu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DATA\\data-bin\\mono exists, will not overwrite!\n"
          ]
        }
      ],
      "source": [
        "binpath = Path('./DATA/data-bin', mono_dataset_name)\n",
        "src_dict_file = './DATA/data-bin/ted2020/dict.en.txt'\n",
        "tgt_dict_file = src_dict_file\n",
        "monopref = str(mono_prefix/\"mono.tok\") # whatever filepath you get after applying subword tokenization\n",
        "if binpath.exists():\n",
        "    print(binpath, \"exists, will not overwrite!\")\n",
        "else:\n",
        "    !python -m fairseq_cli.preprocess\\\n",
        "        --source-lang zh\\\n",
        "        --target-lang en\\\n",
        "        --trainpref \"{monopref}\"\\\n",
        "        --destdir {binpath}\\\n",
        "        --srcdict {src_dict_file}\\\n",
        "        --tgtdict {tgt_dict_file}\\\n",
        "        --workers 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smA0JraEQdxz"
      },
      "source": [
        "### TODO: Generate synthetic data with backward model\n",
        "\n",
        "Add binarized monolingual data to the original data directory, and name it with \"split_name\"\n",
        "\n",
        "ex. ./DATA/data-bin/ted2020/\\[split_name\\].zh-en.\\[\"en\", \"zh\"\\].\\[\"bin\", \"idx\"\\]\n",
        "\n",
        "then you can use 'generate_prediction(model, task, split=\"split_name\")' to generate translation prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "jvaOVHeoQfkB"
      },
      "outputs": [],
      "source": [
        "# # Add binarized monolingual data to the original data directory, and name it with \"split_name\"\n",
        "# # ex. ./DATA/data-bin/ted2020/\\[split_name\\].zh-en.\\[\"en\", \"zh\"\\].\\[\"bin\", \"idx\"\\]\n",
        "!cp ./DATA/data-bin/mono/train.zh-en.zh.bin ./DATA/data-bin/ted2020/mono.zh-en.zh.bin\n",
        "!cp ./DATA/data-bin/mono/train.zh-en.zh.idx ./DATA/data-bin/ted2020/mono.zh-en.zh.idx\n",
        "!cp ./DATA/data-bin/mono/train.zh-en.en.bin ./DATA/data-bin/ted2020/mono.zh-en.en.bin\n",
        "!cp ./DATA/data-bin/mono/train.zh-en.en.idx ./DATA/data-bin/ted2020/mono.zh-en.en.idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "fFEkxPu-Qhlc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-14 17:11:16 | INFO | fairseq.data.data_utils | loaded 781,713 examples from: ./DATA/data-bin/ted2020\\mono.zh-en.zh\n",
            "2023-04-14 17:11:16 | INFO | fairseq.data.data_utils | loaded 781,713 examples from: ./DATA/data-bin/ted2020\\mono.zh-en.en\n",
            "2023-04-14 17:11:16 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020 mono zh-en 781713 examples\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01cc64abc4f247dc94a9bd54ed75014d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "prediction:   0%|          | 0/1715 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# # hint: do prediction on split='mono' to create prediction_file\n",
        "# # generate_prediction( ... ,split=... ,outfile=... )\n",
        "# generate_prediction(model, task, split=\"mono\", outfile=\"./DATA/rawdata/mono/ted_zh_corpus.deduped.clean.en\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn4XeawpQjLk"
      },
      "source": [
        "### TODO: Create new dataset\n",
        "\n",
        "1. Combine the prediction data with monolingual data\n",
        "2. Use the original spm model to tokenize data into Subword Units\n",
        "3. Binarize data with fairseq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "3R35JTaTQjkm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\DATA\\rawdata\\mono\\mono.tok.zh exists. skipping spm_encode.\n",
            "d:\\Wei-shun Bao\\Learning\\Codes\\ML2022-Spring\\HW05\\DATA\\rawdata\\mono\\mono.tok.en exists. skipping spm_encode.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 01:03:23 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='zh', target_lang='en', trainpref='./DATA/rawdata/mono/mono.tok', validpref=None, testpref=None, align_suffix=None, destdir='DATA\\\\data-bin\\\\synthetic', thresholdtgt=0, thresholdsrc=0, tgtdict='./DATA/data-bin/ted2020/dict.en.txt', srcdict='./DATA/data-bin/ted2020/dict.en.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=2, dict_only=False)\n",
            "2023-04-15 01:03:23 | INFO | fairseq_cli.preprocess | [zh] Dictionary: 8000 types\n",
            "2023-04-15 01:05:04 | INFO | fairseq_cli.preprocess | [zh] ./DATA/rawdata/mono/mono.tok.zh: 781713 sents, 14003299 tokens, 0.0023% replaced (by <unk>)\n",
            "2023-04-15 01:05:04 | INFO | fairseq_cli.preprocess | [en] Dictionary: 8000 types\n",
            "2023-04-15 01:06:54 | INFO | fairseq_cli.preprocess | [en] ./DATA/rawdata/mono/mono.tok.en: 781713 sents, 16734005 tokens, 0.0% replaced (by <unk>)\n",
            "2023-04-15 01:06:54 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to DATA\\data-bin\\synthetic\n"
          ]
        }
      ],
      "source": [
        "# Combine prediction_file (.en) and mono.zh (.zh) into a new dataset.\n",
        "# \n",
        "# hint: tokenize prediction_file with the spm model\n",
        "# spm_model.encode(line, out_type=str)\n",
        "# output: ./DATA/rawdata/mono/mono.tok.en & mono.tok.zh\n",
        "\n",
        "spm_model = spm.SentencePieceProcessor(model_file=str(prefix/f'spm{vocab_size}.model'))\n",
        "\n",
        "for lang in [ \"zh\", \"en\" ]:\n",
        "    mono_out_path = mono_prefix / f\"mono.tok.{lang}\"\n",
        "    if mono_out_path.exists():\n",
        "        print(f\"{mono_out_path} exists. skipping spm_encode.\")\n",
        "    else:\n",
        "        with open(mono_out_path, 'w') as out_f:\n",
        "            with open(f'{data_mono_prefix}.clean.{lang}', 'r') as in_f:\n",
        "                for line in in_f:\n",
        "                    line = line.strip()\n",
        "                    tok = spm_model.encode(line, out_type=str)\n",
        "                    print(' '.join(tok), file=out_f)\n",
        "\n",
        "#\n",
        "# hint: use fairseq to binarize these two files again\n",
        "binpath = Path('./DATA/data-bin/synthetic')\n",
        "src_dict_file = './DATA/data-bin/ted2020/dict.en.txt'\n",
        "tgt_dict_file = src_dict_file\n",
        "monopref = \"./DATA/rawdata/mono/mono.tok\" # or whatever path after applying subword tokenization, w/o the suffix (.zh/.en)\n",
        "if binpath.exists():\n",
        "    print(binpath, \"exists, will not overwrite!\")\n",
        "else:\n",
        "    !python -m fairseq_cli.preprocess\\\n",
        "        --source-lang zh\\\n",
        "        --target-lang en\\\n",
        "        --trainpref {monopref}\\\n",
        "        --destdir {binpath}\\\n",
        "        --srcdict {src_dict_file}\\\n",
        "        --tgtdict {tgt_dict_file}\\\n",
        "        --workers 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "MSkse1tyQnsR"
      },
      "outputs": [],
      "source": [
        "# create a new dataset from all the files prepared above\n",
        "!cp -r ./DATA/data-bin/ted2020/ ./DATA/data-bin/ted2020_with_mono/\n",
        "\n",
        "!cp ./DATA/data-bin/synthetic/train.zh-en.zh.bin ./DATA/data-bin/ted2020_with_mono/train1.en-zh.zh.bin\n",
        "!cp ./DATA/data-bin/synthetic/train.zh-en.zh.idx ./DATA/data-bin/ted2020_with_mono/train1.en-zh.zh.idx\n",
        "!cp ./DATA/data-bin/synthetic/train.zh-en.en.bin ./DATA/data-bin/ted2020_with_mono/train1.en-zh.en.bin\n",
        "!cp ./DATA/data-bin/synthetic/train.zh-en.en.idx ./DATA/data-bin/ted2020_with_mono/train1.en-zh.en.idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVdxVGO3QrSs"
      },
      "source": [
        "Created new dataset \"ted2020_with_mono\"\n",
        "\n",
        "1. Change the datadir in **config** (\"./DATA/data-bin/ted2020_with_mono\")\n",
        "2. Switch back the source_lang and target_lang in **config** (\"en\", \"zh\")\n",
        "2. Change the savedir in **config** (eg. \"./checkpoints/transformer-bt\")\n",
        "3. Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CZU2beUQtl3"
      },
      "source": [
        "1. <a name=ott2019fairseq></a>Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., ... & Auli, M. (2019, June). fairseq: A Fast, Extensible Toolkit for Sequence Modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations) (pp. 48-53).\n",
        "2. <a name=vaswani2017></a>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017, December). Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems (pp. 6000-6010).\n",
        "3. <a name=reimers-2020-multilingual-sentence-bert></a>Reimers, N., & Gurevych, I. (2020, November). Making Monolingual Sentence Embeddings Multilingual Using Knowledge Distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 4512-4525).\n",
        "4. <a name=tiedemann2012parallel></a>Tiedemann, J. (2012, May). Parallel Data, Tools and Interfaces in OPUS. In Lrec (Vol. 2012, pp. 2214-2218).\n",
        "5. <a name=kudo-richardson-2018-sentencepiece></a>Kudo, T., & Richardson, J. (2018, November). SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (pp. 66-71).\n",
        "6. <a name=sennrich-etal-2016-improving></a>Sennrich, R., Haddow, B., & Birch, A. (2016, August). Improving Neural Machine Translation Models with Monolingual Data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 86-96).\n",
        "7. <a name=edunov-etal-2018-understanding></a>Edunov, S., Ott, M., Auli, M., & Grangier, D. (2018). Understanding Back-Translation at Scale. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (pp. 489-500).\n",
        "8. https://github.com/ajinkyakulkarni14/TED-Multilingual-Parallel-Corpus\n",
        "9. https://ithelp.ithome.com.tw/articles/10233122\n",
        "10. https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
        "11. https://colab.research.google.com/github/ga642381/ML2021-Spring/blob/main/HW05/HW05.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rrfm6iLJQ0tS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "nKb4u67-sT_Z",
        "n1rwQysTsdJq",
        "59si_C0Wsms7",
        "oOpG4EBRLwe_",
        "6ZlE_1JnMv56",
        "UDAPmxjRNEEL",
        "ce5n4eS7NQNy",
        "rUB9f1WCNgMH",
        "VFJlkOMONsc6",
        "Gt1lX3DRO_yU",
        "BAGMiun8PnZy",
        "JOVQRHzGQU4-",
        "jegH0bvMQVmR",
        "a65glBVXQZiE",
        "smA0JraEQdxz",
        "Jn4XeawpQjLk"
      ],
      "name": "HW05.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "machine-learning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "3b287b2638b28c143e8b740ddcb0a96b617189043e0330c7f965d24699a487e2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
